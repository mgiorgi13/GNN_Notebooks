{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mgiorgi13/GNN_Notebooks/blob/main/Graph%20Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcRdc8jlu-Jj"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rwL9Jpbyvf5Q"
   },
   "outputs": [],
   "source": [
    "#Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# GNN_PATH = '/content/drive/MyDrive/Colab Notebooks/GNN/'\n",
    "\n",
    "#Local\n",
    "GNN_PATH = './GNN/'\n",
    "import os\n",
    "if not os.path.exists(GNN_PATH):\n",
    "    os.makedirs(GNN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RiOQYdHgeZl",
    "outputId": "9ed68555-5f33-487f-f504-4cafc00efcdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch-geometric in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (2.6.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: shapely in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (2.0.7)\n",
      "Requirement already satisfied: seaborn in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: pygmtools in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: numpy in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: moviepy in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: optuna in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: tensorboard in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: filelock in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch-geometric) (3.10.11)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: Pillow>=7.2.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pygmtools) (10.4.0)\n",
      "Requirement already satisfied: easydict>=1.7 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pygmtools) (1.13)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pygmtools) (1.4.4)\n",
      "Requirement already satisfied: async-timeout in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pygmtools) (5.0.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from moviepy) (0.1.11)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from moviepy) (2.35.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: PyYAML in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (2.40.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (5.29.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (49.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (3.0.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from tensorboard) (0.45.1)\n",
      "Requirement already satisfied: Mako in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: importlib-metadata in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (8.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from requests->torch-geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from requests->torch-geometric) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.15.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/matteogiorgi/Github/GNN_Notebooks/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA version: None\n",
      "zsh:1: command not found: nvcc\n",
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "%pip install torch torch-geometric scikit-learn pandas shapely seaborn pygmtools numpy moviepy matplotlib optuna tensorboard\n",
    "#check if pygmtools is installed\n",
    "try:\n",
    "    import pygmtools\n",
    "except ImportError:#pygmtools library\n",
    "    %pip install git+https://github.com/Thinklab-SJTU/pygmtools.git\n",
    "\n",
    "# Check pytorch version and make sure you use a GPU Kernel\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "!nvcc --version\n",
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "#set device as cuda if available to load model and data on gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "argo4tyhdsZF"
   },
   "outputs": [],
   "source": [
    "# ─── Standard library ──────────────────────────────────────────────────────────\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# ─── Third-party libraries ─────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from shapely.affinity import translate\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from typing import Optional, Literal\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import optuna\n",
    "import json\n",
    "\n",
    "# ─── Local application/library imports ────────────────────────────────────────\n",
    "import pygmtools\n",
    "pygmtools.BACKEND = 'pytorch'\n",
    "\n",
    "destination_dir = os.path.join('AFAT')\n",
    "\n",
    "# Ensure the destination directory is in sys.path\n",
    "if destination_dir not in sys.path:\n",
    "    sys.path.append(destination_dir)\n",
    "\n",
    "# AFA-U inlier predictor and Top-K matching from AFAT\n",
    "from k_pred_net import Encoder as AFAUEncoder\n",
    "from sinkhorn_topk import soft_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DrLFFvkZdsZG",
    "outputId": "2c68baad-df49-4dfb-d6cb-9e1586a911c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127706dd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed once at the beginning\n",
    "set_seed(seed)\n",
    "\n",
    "# For reproducible DataLoader shuffle\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLtnXFm_dsZH"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0lWryOMfdsZI"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            DATASET UTILS\n",
    "#----------------------------------------\n",
    "\n",
    "def deserialize_MSD_dataset(data_path, original_path=None, noise_path=None, dimensions_path=None):\n",
    "    dataset_dir = Path(data_path)\n",
    "\n",
    "    dimensions = []\n",
    "    if dimensions_path is not None:\n",
    "        # Load dimensions\n",
    "        dimensions_file = dataset_dir / f\"{dimensions_path}.pickle\"\n",
    "        if not dimensions_file.exists():\n",
    "            raise FileNotFoundError(f\"Dimensions file not found at {dimensions_file}\")\n",
    "        with open(dimensions_file, 'rb') as f:\n",
    "            dimensions = pickle.load(f)\n",
    "\n",
    "    # Clear existing graphs\n",
    "    original = []\n",
    "    noise = []\n",
    "\n",
    "    if original_path is not None:\n",
    "        original_dir = dataset_dir / original_path\n",
    "        original_files = sorted(original_dir.glob(\"*.pt\"), key=lambda f: int(f.stem))\n",
    "        print(f\"Loading {len(original_files)} original graphs...\")\n",
    "        for file in tqdm(original_files, desc=\"Original graphs\"):\n",
    "            with open(str(file), \"rb\") as f:\n",
    "                graph = pickle.load(f)\n",
    "                graph.graph['name'] = file.stem\n",
    "            original.append(graph)\n",
    "\n",
    "    if noise_path is not None:\n",
    "        def extract_numeric_key(file):\n",
    "            \"\"\"Extracts (X, Y) from filenames like 'X_Y.pt' for proper numeric sorting.\"\"\"\n",
    "            name_parts = file.stem.split(\"_\")\n",
    "            return int(name_parts[0]), int(name_parts[1])\n",
    "\n",
    "        noise_dir = dataset_dir / noise_path\n",
    "        noise_files = sorted(noise_dir.glob(\"*.pt\"), key=extract_numeric_key)\n",
    "        print(f\"Loading {len(noise_files)} noise graphs...\")\n",
    "        for file in tqdm(noise_files, desc=\"Noise graphs\"):\n",
    "            with open(str(file), \"rb\") as f:\n",
    "                graph = pickle.load(f)\n",
    "                graph.graph['name'] = file.stem\n",
    "            noise.append(graph)\n",
    "\n",
    "    return original, noise, dimensions\n",
    "\n",
    "def serialize_graph_matching_dataset(pairs: List[Tuple[Data, Data, torch.Tensor]], path: str, filename: str = \"train_dataset.pkl\"):\n",
    "    \"\"\"\n",
    "    Serialize a list of (Data1, Data2, PermutationMatrix) tuples to a file.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(path, filename)\n",
    "\n",
    "    with open(full_path, 'wb') as f:\n",
    "        pickle.dump(pairs, f)\n",
    "\n",
    "    print(f\"Serialized {len(pairs)} pairs to {full_path}\")\n",
    "\n",
    "def deserialize_graph_matching_dataset(path: str, filename: str = \"train_dataset.pkl\") -> List[Tuple[Data, Data, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Deserialize a dataset of (Data1, Data2, PermutationMatrix) tuples from a file.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "\n",
    "    with open(full_path, 'rb') as f:\n",
    "        pairs = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(pairs)} pairs from {full_path}\")\n",
    "    return pairs\n",
    "\n",
    "def plot_a_graph(graphs_list, path=None, viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=False, viz_walls=True):\n",
    "    \"\"\"\n",
    "    Visualizes geometries, wall segments, and graph edges for multiple apartments in 2D.\n",
    "\n",
    "    Parameters:\n",
    "    graphs_list (list of networkx.Graph): List of graphs with nodes ('type', 'center', 'normal') and edges for the apartments.\n",
    "    viz_normals (bool): If True, plots wall segment normals.\n",
    "    viz_rooms (bool): If True, displays room polygons.\n",
    "    viz_ws (bool): If True, displays wall segments.\n",
    "    viz_openings (bool): If True, displays openings (doors and windows).\n",
    "    viz_wall_edges (bool): If True, displays edges between wall segments.\n",
    "    viz_connection_edges (bool): If True, displays edges connecting rooms via openings.\n",
    "    viz_walls (bool): If True, displays wall nodes and their edges.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    legend_added = False  # Flag to ensure the legend is added only once\n",
    "    normal_added = False  # Flag to ensure the \"Normal\" label is added only once\n",
    "\n",
    "    for graphs in graphs_list:\n",
    "        # Visualize room polygons\n",
    "        if viz_rooms:\n",
    "            room_nodes = [n for n, d in graphs.nodes(data=True) if d['type'] == 'room']\n",
    "            for idx, room_node in enumerate(room_nodes):\n",
    "                room_data = graphs.nodes[room_node]\n",
    "                # Plot the polygon\n",
    "                room_polygon = Polygon(room_data['polygon'])\n",
    "                x, y = room_polygon.exterior.xy\n",
    "                ax.plot(x, y, color='black', alpha=0.2, label='Room polygon' if not legend_added and idx == 0 else \"\")\n",
    "                # Draw room centroids\n",
    "                ax.scatter(room_data['center'][0], room_data['center'][1], color='blue', s=100, label='Room centroid' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "        # Visualize wall nodes and edges\n",
    "        if viz_walls:\n",
    "            wall_nodes = [n for n, d in graphs.nodes(data=True) if d['type'] == 'wall']\n",
    "            for idx, wn in enumerate(wall_nodes):\n",
    "                wall_data = graphs.nodes[wn]\n",
    "                # Plot the polygon of the wall\n",
    "                wall_polygon = Polygon(wall_data['polygon'])\n",
    "                x, y = wall_polygon.exterior.xy\n",
    "                ax.plot(x, y, color='purple', linestyle='-', label='Wall polygon' if not legend_added and idx == 0 else \"\")\n",
    "                ax.scatter(wall_data['center'][0], wall_data['center'][1], color='purple', s=50, label='Wall centroid' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "            if viz_normals:\n",
    "                wall_ws = [n for n, d in graphs.nodes(data=True) if d['type'] == 'wall_ws']\n",
    "                for idx, wn in enumerate(wall_ws):\n",
    "                    ws_data = graphs.nodes[wn]\n",
    "                    ax.scatter(ws_data['center'][0], ws_data['center'][1], color='purple', s=20, label='Wall ws' if not legend_added and idx == 0 else \"\")\n",
    "                    ax.arrow(ws_data['center'][0], ws_data['center'][1],\n",
    "                             ws_data['normal'][0], ws_data['normal'][1],\n",
    "                             head_width=0.1, head_length=0.1, fc='green', ec='green', label='Normal' if not normal_added else \"\")\n",
    "                    normal_added = True\n",
    "\n",
    "            wall_edges = [(u, v) for u, v, d in graphs.edges(data=True) if 'wall' in u or 'wall' in v]\n",
    "            for idx, edge in enumerate(wall_edges):\n",
    "                start_node = graphs.nodes[edge[0]]\n",
    "                end_node = graphs.nodes[edge[1]]\n",
    "                ax.plot([start_node['center'][0], end_node['center'][0]],\n",
    "                        [start_node['center'][1], end_node['center'][1]],\n",
    "                        color='purple', linestyle='--', label='Wall edge' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "        # Visualize openings\n",
    "        if viz_openings:\n",
    "            opening_nodes = [n for n, d in graphs.nodes(data=True) if 'door' in d['type'] or 'window' in d['type']]\n",
    "            for idx, on in enumerate(opening_nodes):\n",
    "                opening_data = graphs.nodes[on]\n",
    "                opening_polygon = Polygon(opening_data['polygon'])\n",
    "                x, y = opening_polygon.exterior.xy\n",
    "                ax.plot(x, y, color='orange', label='Opening polygon' if not legend_added and idx == 0 else \"\")\n",
    "                # Draw opening centroids\n",
    "                ax.scatter(opening_data['center'][0], opening_data['center'][1], color='orange', s=10, label='Opening centroid' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "            if viz_normals:\n",
    "                opening_ws = [n for n, d in graphs.nodes(data=True) if d['type'] == 'door_ws' or d['type'] == 'window_ws']\n",
    "                for idx, wn in enumerate(opening_ws):\n",
    "                    ws_data = graphs.nodes[wn]\n",
    "                    ax.scatter(ws_data['center'][0], ws_data['center'][1], color='orange', s=10, label='Opening ws' if not legend_added and idx == 0 else \"\")\n",
    "                    ax.arrow(ws_data['center'][0], ws_data['center'][1],\n",
    "                             ws_data['normal'][0], ws_data['normal'][1],\n",
    "                             head_width=0.1, head_length=0.1, fc='green', ec='green', label='Normal' if not normal_added else \"\")\n",
    "                    normal_added = True\n",
    "\n",
    "            # Draw opening edges\n",
    "            open_edges = [(u, v) for u, v, d in graphs.edges(data=True) if 'door' in u or 'window' in v or 'door' in v or 'window' in u]\n",
    "            for idx, edge in enumerate(open_edges):\n",
    "                start_node = graphs.nodes[edge[0]]\n",
    "                end_node = graphs.nodes[edge[1]]\n",
    "                ax.plot([start_node['center'][0], end_node['center'][0]],\n",
    "                        [start_node['center'][1], end_node['center'][1]],\n",
    "                        color='orange', linestyle='--', label='Opening edge' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "        # Visualize ws room\n",
    "        if viz_ws:\n",
    "            ws_nodes = [n for n, d in graphs.nodes(data=True) if d['type'] == 'ws']\n",
    "            for idx, wn in enumerate(ws_nodes):\n",
    "                ws_data = graphs.nodes[wn]\n",
    "                ax.scatter(ws_data['center'][0], ws_data['center'][1], color='red', s=20, label='Ws segment' if not legend_added and idx == 0 else \"\")\n",
    "                if viz_room_normals:\n",
    "                    ax.arrow(ws_data['center'][0], ws_data['center'][1],\n",
    "                             ws_data['normal'][0], ws_data['normal'][1],\n",
    "                             head_width=0.1, head_length=0.1, fc='green', ec='green', label='Normal' if not normal_added else \"\")\n",
    "                    normal_added = True\n",
    "                if 'limits' in ws_data:\n",
    "                    limit_1, limit_2 = ws_data['limits']\n",
    "                    ax.plot([limit_1[0], limit_2[0]],\n",
    "                            [limit_1[1], limit_2[1]],\n",
    "                            color='black', linewidth=1.0,\n",
    "                            label='Ws limits' if idx == 0 else \"\")\n",
    "            ws_edges = [(u, v) for u, v, d in graphs.edges(data=True) if 'ws_same_room' in d['type'] or 'ws_belongs_room' in d['type']]\n",
    "            for idx, edge in enumerate(ws_edges):\n",
    "                start_node = graphs.nodes[edge[0]]\n",
    "                end_node = graphs.nodes[edge[1]]\n",
    "                ax.plot([start_node['center'][0], end_node['center'][0]],\n",
    "                    [start_node['center'][1], end_node['center'][1]],\n",
    "                    color='gray', linestyle='--', label='Ws edge' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "        # Visualize connection edges\n",
    "        if viz_room_connection:\n",
    "            connection_edges = [(u, v) for u, v, d in graphs.edges(data=True) if 'connected' in d['type']]\n",
    "            for idx, edge in enumerate(connection_edges):\n",
    "                start_node = graphs.nodes[edge[0]]\n",
    "                end_node = graphs.nodes[edge[1]]\n",
    "                ax.plot([start_node['center'][0], end_node['center'][0]],\n",
    "                        [start_node['center'][1], end_node['center'][1]],\n",
    "                        color='blue', linestyle='-', label='Connection edge' if not legend_added and idx == 0 else \"\")\n",
    "\n",
    "        legend_added = True  # Set the flag to True after processing the first graph\n",
    "\n",
    "    plt.title(\"Apartment Graph Visualization\")\n",
    "    plt.legend()\n",
    "    if path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "node_type_mapping = {\"room\": [1, 0], \"ws\": [0, 1]}\n",
    "\n",
    "def pyg_data_to_nx_digraph(data: Data, graph_list: List[nx.DiGraph]) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Convert a PyTorch Geometric Data object back to a NetworkX DiGraph,\n",
    "    restoring original node IDs using data.node_names and data.permutation,\n",
    "    matching with the graph in graph_list that has the same name.\n",
    "    \"\"\"\n",
    "    assert hasattr(data, 'node_names'), \\\n",
    "        \"Data object must contain 'node_names' to restore original node IDs.\"\n",
    "    assert hasattr(data, 'permutation'), \\\n",
    "        \"Data object must contain 'permutation' to reorder nodes.\"\n",
    "    assert hasattr(data, 'name'), \\\n",
    "        \"Data object must contain 'name' to match with graph_list.\"\n",
    "\n",
    "    matching_graph = next((g for g in graph_list if g.graph.get('name') == data.name), None)\n",
    "    if matching_graph is None:\n",
    "        raise ValueError(f\"No graph with name {data.name} found in graph_list.\")\n",
    "\n",
    "    orig_names = data.node_names\n",
    "    perm = data.permutation.tolist()\n",
    "    node_ids = [orig_names[idx] for idx in perm]\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for node_id in node_ids:\n",
    "        if node_id in matching_graph.nodes:\n",
    "            G.add_node(node_id, **matching_graph.nodes[node_id])\n",
    "\n",
    "    for u_idx, v_idx in data.edge_index.t().tolist():\n",
    "        u = node_ids[u_idx]\n",
    "        v = node_ids[v_idx]\n",
    "        if matching_graph.has_edge(u, v):\n",
    "            G.add_edge(u, v, **matching_graph.edges[u, v])\n",
    "\n",
    "    G.graph['name'] = data.name\n",
    "    return G\n",
    "\n",
    "\n",
    "def nx_to_pyg_data_preserve_order(graph: nx.DiGraph) -> Data:\n",
    "    \"\"\"\n",
    "    Convert a NetworkX DiGraph to a PyTorch Geometric Data object,\n",
    "    preserving node insertion order, storing 'node_names' and an identity 'permutation'.\n",
    "    \"\"\"\n",
    "    node_ids = list(graph.nodes())\n",
    "    id_map = {nid: i for i, nid in enumerate(node_ids)}\n",
    "\n",
    "    x = torch.stack([\n",
    "        torch.tensor(\n",
    "            node_type_mapping[graph.nodes[n]['type']] +\n",
    "            graph.nodes[n]['center'] +\n",
    "            graph.nodes[n]['normal'] +\n",
    "            [graph.nodes[n].get('length', -1)],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        for n in node_ids\n",
    "    ])\n",
    "\n",
    "    edge_index = torch.tensor(\n",
    "        [[id_map[u], id_map[v]] for u, v in graph.edges()],\n",
    "        dtype=torch.long\n",
    "    ).t().contiguous() if graph.edges else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    data.name = graph.graph.get('name')\n",
    "    data.node_names = node_ids\n",
    "    data.permutation = torch.arange(len(node_ids), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def split_graphs_stratified(\n",
    "    pairs: List[Tuple[Data, Data, torch.Tensor]],\n",
    "    train_frac: float = 0.7,\n",
    "    val_frac: float   = 0.15,\n",
    "    test_frac: float  = 0.15,\n",
    "    n_bins: int       = 5,\n",
    "    seed: int         = seed,\n",
    "    stratify_on: str  = \"g2\"     # \"g1\" oppure \"g2\"\n",
    ") -> Tuple[\n",
    "    List[Tuple[Data,Data,torch.Tensor]],\n",
    "    List[Tuple[Data,Data,torch.Tensor]],\n",
    "    List[Tuple[Data,Data,torch.Tensor]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Stratified split of graph-matching pairs into train/val/test.\n",
    "    Puoi stratificare sulla dimensione di g1 o di g2.\n",
    "    \"\"\"\n",
    "    assert stratify_on in (\"g1\",\"g2\"), \"stratify_on must be 'g1' or 'g2'\"\n",
    "    total = train_frac + val_frac + test_frac\n",
    "    assert abs(total - 1.0) < 1e-6, \"train+val+test fractions must sum to 1.0\"\n",
    "\n",
    "    # scegli la dimensione su cui stratificare\n",
    "    if stratify_on == \"g1\":\n",
    "        sizes = np.array([g1.num_nodes for g1, g2, P in pairs])\n",
    "    else:\n",
    "        sizes = np.array([g2.num_nodes for g1, g2, P in pairs])\n",
    "\n",
    "    # quantile‑binning per equal‑frequency\n",
    "    while n_bins > 1:\n",
    "        try:\n",
    "            size_bins = pd.qcut(sizes, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "        except ValueError:\n",
    "            n_bins -= 1\n",
    "            continue\n",
    "        counts = np.bincount(size_bins, minlength=n_bins)\n",
    "        if np.all(counts >= 2):\n",
    "            break\n",
    "        n_bins -= 1\n",
    "\n",
    "    idx = np.arange(len(pairs))\n",
    "    if n_bins <= 1:\n",
    "        # fallback random split\n",
    "        train_idx, temp_idx = train_test_split(idx, test_size=1-train_frac, random_state=seed)\n",
    "        rel_val = val_frac/(val_frac+test_frac)\n",
    "        val_idx, test_idx = train_test_split(temp_idx, test_size=1-rel_val, random_state=seed)\n",
    "    else:\n",
    "        train_idx, temp_idx = train_test_split(\n",
    "            idx, test_size=(1.0-train_frac),\n",
    "            random_state=seed, stratify=size_bins\n",
    "        )\n",
    "        rel_val = val_frac/(val_frac+test_frac)\n",
    "        temp_bins = size_bins[temp_idx]\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            temp_idx, test_size=(1.0-rel_val),\n",
    "            random_state=seed, stratify=temp_bins\n",
    "        )\n",
    "\n",
    "    train = [pairs[i] for i in train_idx]\n",
    "    val   = [pairs[i] for i in val_idx]\n",
    "    test  = [pairs[i] for i in test_idx]\n",
    "    return train, val, test\n",
    "\n",
    "# Controllo rapido delle distribuzioni\n",
    "def describe(split, name):\n",
    "    sz = [g1.num_nodes for g1, _, _ in split]\n",
    "    print(f\"{name}: count={len(split)}, nodes min={min(sz)}, max={max(sz)}, mean={np.mean(sz):.1f}\")\n",
    "\n",
    "def generate_matching_pair_as_data(\n",
    "    g1: nx.DiGraph,\n",
    "    g2: nx.DiGraph,\n",
    "    pairs_list: List[Tuple[Data, Data, torch.Tensor]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate a matching pair for partial graph matching:\n",
    "    - g1: complete graph (reference)\n",
    "    - g2: partial graph to be permuted\n",
    "    Stores (Data_g1, Data_g2_permuted, P) in pairs_list, where P is ground truth of shape [|g1|, |g2|].\n",
    "    \"\"\"\n",
    "    # Convert reference graph\n",
    "    pyg_g1 = nx_to_pyg_data_preserve_order(g1)\n",
    "\n",
    "    # Prepare original names and permutation for g2\n",
    "    orig_names = list(g2.nodes())\n",
    "    num_g1 = g1.number_of_nodes()\n",
    "    num_g2 = len(orig_names)\n",
    "    perm_indices = torch.randperm(num_g2)\n",
    "\n",
    "    # Build permuted g2\n",
    "    g2_perm = nx.DiGraph()\n",
    "    g2_perm.graph['name'] = g2.graph.get('name', '')\n",
    "    for new_idx, orig_idx in enumerate(perm_indices.tolist()):\n",
    "        orig_id = orig_names[orig_idx]\n",
    "        g2_perm.add_node(new_idx, **g2.nodes[orig_id])\n",
    "    # Remap edges\n",
    "    orig_to_new = {orig_names[idx]: new for new, idx in enumerate(perm_indices.tolist())}\n",
    "    for u, v, data_edge in g2.edges(data=True):\n",
    "        if u in orig_to_new and v in orig_to_new:\n",
    "            g2_perm.add_edge(orig_to_new[u], orig_to_new[v], **data_edge)\n",
    "\n",
    "    # Convert permuted graph and attach metadata\n",
    "    pyg_g2 = nx_to_pyg_data_preserve_order(g2_perm)\n",
    "    pyg_g2.permutation = perm_indices\n",
    "    pyg_g2.node_names = orig_names\n",
    "\n",
    "    # Build partial assignment ground truth P [|g1| x |g2|]\n",
    "    P = torch.zeros((num_g1, num_g2), dtype=torch.float32)\n",
    "    g1_ids = list(g1.nodes())\n",
    "    # For each permuted node in g2, find matching index in g1\n",
    "    for j, orig_idx in enumerate(perm_indices.tolist()):\n",
    "        orig_id = orig_names[orig_idx]\n",
    "        if orig_id in g1_ids:\n",
    "            i = g1_ids.index(orig_id)\n",
    "            P[i, j] = 1.0\n",
    "\n",
    "    # Append without transpose to keep shape [|g1|, |g2|]\n",
    "    pairs_list.append((pyg_g1, pyg_g2, P))\n",
    "\n",
    "\n",
    "def plot_two_graphs_with_matching(graphs_list, gt_perm, original_graphs, path=None, noise_graphs=None, pred_perm=None,\n",
    "                                  viz_rooms=True, viz_ws=True,\n",
    "                                  viz_room_connection=True,\n",
    "                                  viz_normals=False, viz_room_normals=False,\n",
    "                                  match_display=\"all\"):\n",
    "    assert match_display in {\"all\", \"correct\", \"wrong\"}, \"match_display must be one of: 'all', 'correct', 'wrong'\"\n",
    "    assert len(graphs_list) == 2, \"graphs_list must contain exactly two graphs.\"\n",
    "    if noise_graphs is None:\n",
    "        noise_graphs = original_graphs\n",
    "\n",
    "    # Extract tensors and original node order\n",
    "    g1tensor, g2tensor = copy.deepcopy(graphs_list[0]), copy.deepcopy(graphs_list[1])\n",
    "    # Node names for g1 in original order\n",
    "    node_names1 = list(g1tensor.node_names)\n",
    "    # Reconstruct node names for g2 according to its permutation\n",
    "    orig_names2 = list(g2tensor.node_names)\n",
    "    perm = g2tensor.permutation.tolist()\n",
    "    node_names2 = [orig_names2[p] for p in perm]\n",
    "\n",
    "    # Convert to NetworkX\n",
    "    g1 = copy.deepcopy(pyg_data_to_nx_digraph(g1tensor, original_graphs))\n",
    "    g2_original = copy.deepcopy(pyg_data_to_nx_digraph(g2tensor, noise_graphs))\n",
    "    g2 = g2_original.copy()\n",
    "\n",
    "    # Translate g2 for side-by-side plot\n",
    "    max_x_g1 = max(data['center'][0] for _, data in g1.nodes(data=True))\n",
    "    min_x_g2 = min(data['center'][0] for _, data in g2.nodes(data=True))\n",
    "    translation_x = (max_x_g1 - min_x_g2) + 10.0\n",
    "    for _, data in g2.nodes(data=True):\n",
    "        data['center'][0] += translation_x\n",
    "        if 'polygon' in data:\n",
    "            poly = data['polygon']\n",
    "            if isinstance(poly, Polygon):\n",
    "                data['polygon'] = translate(poly, xoff=translation_x)\n",
    "            else:\n",
    "                data['polygon'] = Polygon([(x + translation_x, y) for x, y in poly])\n",
    "        if 'limits' in data:\n",
    "            data['limits'] = [[x + translation_x, y] for x, y in data['limits']]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    legend_added = set()\n",
    "\n",
    "    def plot_graph(g, is_g1):\n",
    "        color_room = 'lightblue' if is_g1 else 'navajowhite'\n",
    "        color_ws = 'red' if is_g1 else 'purple'\n",
    "        prefix = \"(G1)\" if is_g1 else \"(G2)\"\n",
    "\n",
    "        if viz_rooms:\n",
    "            for n, d in g.nodes(data=True):\n",
    "                if d['type'] == 'room' and 'polygon' in d:\n",
    "                    poly = Polygon(d['polygon']) if not isinstance(d['polygon'], Polygon) else d['polygon']\n",
    "                    x, y = poly.exterior.xy\n",
    "                    ax.fill(x, y, color=color_room, alpha=0.3,\n",
    "                            label=f\"Room polygon {prefix}\" if f\"room-poly-{prefix}\" not in legend_added else \"\")\n",
    "                    ax.scatter(d['center'][0], d['center'][1], color='blue', s=80,\n",
    "                               label=f\"Centroid {prefix}\" if f\"room-pt-{prefix}\" not in legend_added else \"\")\n",
    "                    legend_added.update({f\"room-poly-{prefix}\", f\"room-pt-{prefix}\"})\n",
    "\n",
    "        if viz_ws:\n",
    "            for n, d in g.nodes(data=True):\n",
    "                if d['type'] == 'ws':\n",
    "                    ax.scatter(d['center'][0], d['center'][1], color=color_ws, s=20,\n",
    "                               label=f\"WS {prefix}\" if f\"ws-{prefix}\" not in legend_added else \"\")\n",
    "                    legend_added.add(f\"ws-{prefix}\")\n",
    "                    if 'limits' in d:\n",
    "                        limit1, limit2 = d['limits']\n",
    "                        ax.plot([limit1[0], limit2[0]], [limit1[1], limit2[1]],\n",
    "                                color='black', linewidth=1.0,\n",
    "                                label=f\"WS limits {prefix}\" if f\"limits-{prefix}\" not in legend_added else \"\")\n",
    "                        legend_added.add(f\"limits-{prefix}\")\n",
    "\n",
    "    plot_graph(g1, is_g1=True)\n",
    "    plot_graph(g2, is_g1=False)\n",
    "\n",
    "    # Plot matching lines with partial-match and ID presence checks\n",
    "    if pred_perm is not None:\n",
    "        for i in range(pred_perm.shape[0]):  # for each row\n",
    "            # skip if ground truth has no assignment for this node\n",
    "            if gt_perm[i].sum().item() == 0:\n",
    "                continue\n",
    "            row = pred_perm[i]\n",
    "            # determine if prediction exists\n",
    "            if row.sum().item() == 0:\n",
    "                # missing prediction: draw based on ground truth\n",
    "                j_gt = gt_perm[i].argmax().item()\n",
    "                # map indices to node IDs\n",
    "                id1 = node_names1[i]\n",
    "                if id1 not in g1.nodes:\n",
    "                    continue\n",
    "                if j_gt < len(node_names2):\n",
    "                    id2 = node_names2[j_gt]\n",
    "                else:\n",
    "                    continue\n",
    "                if id2 not in g2.nodes:\n",
    "                    continue\n",
    "                pt1 = g1.nodes[id1]['center']\n",
    "                pt2 = g2.nodes[id2]['center']\n",
    "                # skip if match_display filters out missing\n",
    "                if match_display in {\"correct\", \"wrong\"}:\n",
    "                    continue\n",
    "                color = 'yellow'\n",
    "                label = None\n",
    "                if 'missing' not in legend_added:\n",
    "                    label = 'Missing match'\n",
    "                    legend_added.add('missing')\n",
    "                ax.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]],\n",
    "                        color=color, linestyle='--', alpha=0.6, linewidth=1, label=label)\n",
    "                continue\n",
    "            # has prediction: handle correct/wrong\n",
    "            j = row.argmax().item()\n",
    "            # map indices to node IDs\n",
    "            id1 = node_names1[i]\n",
    "            if id1 not in g1.nodes:\n",
    "                continue\n",
    "            if j < len(node_names2):\n",
    "                id2 = node_names2[j]\n",
    "            else:\n",
    "                continue\n",
    "            if id2 not in g2.nodes:\n",
    "                continue\n",
    "            pt1 = g1.nodes[id1]['center']\n",
    "            pt2 = g2.nodes[id2]['center']\n",
    "            is_correct = (j < gt_perm.shape[1] and gt_perm[i, j] == 1)\n",
    "            if match_display == \"correct\" and not is_correct:\n",
    "                continue\n",
    "            if match_display == \"wrong\" and is_correct:\n",
    "                continue\n",
    "            color = 'green' if is_correct else 'red'\n",
    "            label = None\n",
    "            if color == 'green' and 'correct' not in legend_added:\n",
    "                label = 'Correct match'\n",
    "                legend_added.add('correct')\n",
    "            elif color == 'red' and 'wrong' not in legend_added:\n",
    "                label = 'Wrong match'\n",
    "                legend_added.add('wrong')\n",
    "            ax.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]],\n",
    "                    color=color, linestyle='-', alpha=0.6, linewidth=1, label=label)\n",
    "\n",
    "    ax.set_title(\"Graph Matching: Green = Correct, Red = Wrong\")\n",
    "    ax.axis(\"equal\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "def normalize_data_pairs(\n",
    "    pairs: List[Tuple[Data, Data, torch.Tensor]],\n",
    "    mean: torch.Tensor,\n",
    "    std: torch.Tensor\n",
    ") -> List[Tuple[Data, Data, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Normalizza per-feature i tensori x in ciascun Data object all'interno delle tuple.\n",
    "\n",
    "    Args:\n",
    "        pairs: Lista di tuple (Data1, Data2, P)\n",
    "        mean: Tensor di media per-feature (shape: [num_features])\n",
    "        std: Tensor di deviazione standard per-feature (shape: [num_features])\n",
    "\n",
    "    Returns:\n",
    "        Lista di tuple con i Data normalizzati.\n",
    "    \"\"\"\n",
    "    normalized_pairs = []\n",
    "    for data1, data2, P in pairs:\n",
    "        data1.x = (data1.x - mean) / (std + 1e-8)\n",
    "        data2.x = (data2.x - mean) / (std + 1e-8)\n",
    "        normalized_pairs.append((data1, data2, P))\n",
    "    return normalized_pairs\n",
    "\n",
    "def compute_mean_std(pairs: List[Tuple[Data, Data, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calcola la media e la deviazione standard per-feature dai Data objects nel training set.\n",
    "\n",
    "    Args:\n",
    "        pairs: Lista di tuple (Data1, Data2, P) del training set\n",
    "\n",
    "    Returns:\n",
    "        Tuple contenente (mean, std) per-feature\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    for data1, data2, _ in pairs:\n",
    "        x_list.append(data1.x)\n",
    "        x_list.append(data2.x)\n",
    "    x_all = torch.cat(x_list, dim=0)\n",
    "    mean = x_all.mean(dim=0)\n",
    "    std = x_all.std(dim=0)\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            LOGGING\n",
    "#----------------------------------------\n",
    "\n",
    "# Generic logger setup for any model/dataset\n",
    "def setup_tb_logger(\n",
    "    base_dir: str = \"runs\",\n",
    "    model_name: str = None,\n",
    "    dataset_name: str = None,\n",
    "    experiment_name: str = None\n",
    ") -> SummaryWriter:\n",
    "    \"\"\"\n",
    "    Create a TensorBoard SummaryWriter with a structured log directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir: root directory for all runs.\n",
    "        model_name: identifier for the model (e.g. \"GATv2\", \"MyModel\").\n",
    "        dataset_name: identifier for the dataset (e.g. \"CIFAR10\").\n",
    "        experiment_name: optional extra tag (e.g. \"dropout0.3\").\n",
    "\n",
    "    Returns:\n",
    "        writer: a SummaryWriter instance logging to runs/... directory.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if model_name:\n",
    "        parts.append(model_name)\n",
    "    if dataset_name:\n",
    "        parts.append(dataset_name)\n",
    "    if experiment_name:\n",
    "        parts.append(experiment_name)\n",
    "    # timestamp for uniqueness\n",
    "    parts.append(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    log_dir = os.path.join(base_dir, \"__\".join(parts))\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    return writer\n",
    "\n",
    "\n",
    "def log_gradients(\n",
    "    writer: SummaryWriter,\n",
    "    model: torch.nn.Module,\n",
    "    epoch: int,\n",
    "    prefix: str = \"grad_norms\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Log the L2 norm of gradients of all parameters in the model.\n",
    "\n",
    "    Args:\n",
    "        writer: SummaryWriter returned by setup_tb_logger.\n",
    "        model: the neural network model whose gradients to log.\n",
    "        epoch: current epoch or step index.\n",
    "        prefix: prefix for the TensorBoard tags.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            writer.add_scalar(f\"{prefix}/{name}\", param.grad.norm().item(), epoch)\n",
    "\n",
    "\n",
    "def log_metrics(\n",
    "    writer: SummaryWriter,\n",
    "    metrics: Dict[str, float],\n",
    "    epoch: int,\n",
    "    prefix: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Log arbitrary metrics (e.g. losses, accuracies) to TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        writer: SummaryWriter returned by setup_tb_logger.\n",
    "        metrics: dict of metric_name -> value.\n",
    "        epoch: current epoch or step index.\n",
    "        prefix: optional prefix for tags (e.g. \"train\", \"val\").\n",
    "    \"\"\"\n",
    "    for key, value in metrics.items():\n",
    "        tag = f\"{prefix}/{key}\" if prefix else key\n",
    "        writer.add_scalar(tag, value, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            TRAINING UTILS\n",
    "#----------------------------------------\n",
    "\n",
    "# Create the plot\n",
    "def plot_losses(train_losses, val_losses, output_path=None):\n",
    "    epochs = list(range(len(train_losses)))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(x=epochs, y=train_losses, label=\"Training Loss\")\n",
    "    sns.lineplot(x=epochs, y=val_losses, label=\"Validation Loss\")\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Save the plot to the specified path\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "class GraphMatchingDataset(Dataset):\n",
    "    def __init__(self, pairs):  # lista di (Data, Data, P)\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]  # data1, data2, P\n",
    "\n",
    "def collate_pyg_matching(batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    data1_list, data2_list, perm_list = zip(*batch)\n",
    "    \n",
    "    # Sposta ogni grafo sul device corretto\n",
    "    data1_list = [d.to(device) for d in data1_list]\n",
    "    data2_list = [d.to(device) for d in data2_list]\n",
    "    \n",
    "    batch1 = Batch.from_data_list(data1_list)\n",
    "    batch2 = Batch.from_data_list(data2_list)\n",
    "    \n",
    "    return batch1, batch2, perm_list\n",
    "\n",
    "### FUNCTIONS WITH COLUMN-WISE CE\n",
    "# def train_epoch_sinkhorn(model, loader, optimizer, writer, epoch, eps: float = 1e-9):\n",
    "#     \"\"\"\n",
    "#     Trains one epoch of a Sinkhorn-based graph matching model using column-wise cross-entropy loss.\n",
    "#     Returns:\n",
    "#         avg_loss (float): average column CE loss per batch.\n",
    "#         all_embeddings (list): collected embeddings from the model.\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     all_embeddings = []\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     for batch1, batch2, perm_list in loader:\n",
    "#         batch1 = batch1.to(device)\n",
    "#         batch2 = batch2.to(device)\n",
    "#         perm_list = [p.to(device) for p in perm_list]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         batch_idx1 = batch1.batch\n",
    "#         batch_idx2 = batch2.batch\n",
    "#         pred_perm_list, batch_embeddings = model(batch1, batch2, batch_idx1, batch_idx2)\n",
    "\n",
    "#         # column-wise cross-entropy\n",
    "#         batch_loss = 0.0\n",
    "#         for P, P_gt in zip(pred_perm_list, perm_list):\n",
    "#             # For each column j: -sum_i P_gt[i,j] * log(P[i,j])\n",
    "#             ce_per_col = -torch.sum(P_gt * torch.log(P + eps), dim=0)\n",
    "#             batch_loss += ce_per_col.mean()\n",
    "#         batch_loss = batch_loss / len(pred_perm_list)\n",
    "\n",
    "#         batch_loss.backward()\n",
    "#         # Log gradients\n",
    "#         log_gradients(writer, model, epoch)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += batch_loss.item()\n",
    "#         all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "#     return avg_loss, all_embeddings\n",
    "\n",
    "\n",
    "# def evaluate_sinkhorn(model, loader, eps: float = 1e-9):\n",
    "#     \"\"\"\n",
    "#     Evaluates a Sinkhorn-based graph matching model using column-wise cross-entropy\n",
    "#     and permutation accuracy.\n",
    "#     Returns:\n",
    "#         avg_acc (float): permutation accuracy over all columns.\n",
    "#         avg_loss (float): average column CE loss per example.\n",
    "#         all_embeddings (list): collected embeddings from the model.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total_cols = 0\n",
    "#     total_loss = 0.0\n",
    "#     num_graphs = 0\n",
    "#     all_embeddings = []\n",
    "\n",
    "#     device = next(model.parameters()).device\n",
    "#     with torch.no_grad():\n",
    "#         for batch1, batch2, perm_list in loader:\n",
    "#             batch1 = batch1.to(device)\n",
    "#             batch2 = batch2.to(device)\n",
    "#             perm_list = [p.to(device) for p in perm_list]\n",
    "\n",
    "#             batch_idx1 = batch1.batch\n",
    "#             batch_idx2 = batch2.batch\n",
    "#             pred_perm_list, batch_embeddings = model(batch1, batch2, batch_idx1, batch_idx2)\n",
    "\n",
    "#             for P, P_gt in zip(pred_perm_list, perm_list):\n",
    "#                 # permutation accuracy: column-wise argmax compare\n",
    "#                 pred_idx = P.argmax(dim=0)\n",
    "#                 target_idx = P_gt.argmax(dim=0)\n",
    "#                 correct += (pred_idx == target_idx).sum().item()\n",
    "#                 total_cols += P.shape[1]\n",
    "\n",
    "#                 # column-wise CE loss\n",
    "#                 ce_per_col = -torch.sum(P_gt * torch.log(P + eps), dim=0)\n",
    "#                 total_loss += ce_per_col.mean().item()\n",
    "#                 num_graphs += 1\n",
    "\n",
    "#             all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "#     avg_acc = correct / total_cols if total_cols > 0 else 0.0\n",
    "#     avg_loss = total_loss / num_graphs if num_graphs > 0 else 0.0\n",
    "#     return avg_acc, avg_loss, all_embeddings\n",
    "\n",
    "### FUNCTIONS WITH BCE\n",
    "def bce_permutation_loss(P, P_gt, eps: float = 1e-9):\n",
    "    \"\"\"Element-wise Binary Cross Entropy loss between prediction and ground truth.\"\"\"\n",
    "    return - (P_gt * torch.log(P + eps) + (1 - P_gt) * torch.log(1 - P + eps)).mean()\n",
    "\n",
    "def train_epoch_sinkhorn(model, loader, optimizer, writer, epoch, eps: float = 1e-9):\n",
    "    \"\"\"\n",
    "    Trains one epoch of a Sinkhorn-based graph matching model using Binary Cross Entropy (BCE) loss.\n",
    "    Returns:\n",
    "        avg_loss (float): average BCE loss per graph.\n",
    "        all_embeddings (list): collected embeddings from the model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_graphs = 0\n",
    "    all_embeddings = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch1, batch2, perm_list in loader:\n",
    "        batch1 = batch1.to(device)\n",
    "        batch2 = batch2.to(device)\n",
    "        perm_list = [p.to(device) for p in perm_list]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_idx1 = batch1.batch\n",
    "        batch_idx2 = batch2.batch\n",
    "        pred_perm_list, batch_embeddings = model(batch1, batch2, batch_idx1, batch_idx2)\n",
    "\n",
    "        # accumulo loss per grafo\n",
    "        batch_loss = 0.0\n",
    "        for P, P_gt in zip(pred_perm_list, perm_list):\n",
    "            loss = bce_permutation_loss(P, P_gt, eps)  # assume reduction='mean'\n",
    "            batch_loss += loss\n",
    "            total_loss += loss.item()\n",
    "            num_graphs += 1\n",
    "\n",
    "        batch_loss = batch_loss / len(pred_perm_list)  # per logging/grad\n",
    "        batch_loss.backward()\n",
    "        log_gradients(writer, model, epoch)\n",
    "        optimizer.step()\n",
    "\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    avg_loss = total_loss / num_graphs if num_graphs > 0 else 0.0\n",
    "    return avg_loss, all_embeddings\n",
    "\n",
    "\n",
    "def evaluate_sinkhorn(model, loader, eps: float = 1e-9):\n",
    "    \"\"\"\n",
    "    Evaluates a Sinkhorn-based graph matching model using Binary Cross Entropy\n",
    "    and permutation accuracy.\n",
    "    Returns:\n",
    "        avg_acc (float): permutation accuracy over all columns.\n",
    "        avg_loss (float): average BCE loss per graph.\n",
    "        all_embeddings (list): collected embeddings from the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_cols = 0\n",
    "    total_loss = 0.0\n",
    "    num_graphs = 0\n",
    "    all_embeddings = []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, perm_list in loader:\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            perm_list = [p.to(device) for p in perm_list]\n",
    "\n",
    "            batch_idx1 = batch1.batch\n",
    "            batch_idx2 = batch2.batch\n",
    "            pred_perm_list, batch_embeddings = model(batch1, batch2, batch_idx1, batch_idx2)\n",
    "\n",
    "            for P, P_gt in zip(pred_perm_list, perm_list):\n",
    "                # accuracy\n",
    "                pred_idx = P.argmax(dim=0)\n",
    "                target_idx = P_gt.argmax(dim=0)\n",
    "                correct += (pred_idx == target_idx).sum().item()\n",
    "                total_cols += P.shape[1]\n",
    "\n",
    "                # loss per grafo\n",
    "                loss = bce_permutation_loss(P, P_gt, eps)\n",
    "                total_loss += loss.item()\n",
    "                num_graphs += 1\n",
    "\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    avg_acc = correct / total_cols if total_cols > 0 else 0.0\n",
    "    avg_loss = total_loss / num_graphs if num_graphs > 0 else 0.0\n",
    "    return avg_acc, avg_loss, all_embeddings\n",
    "\n",
    "\n",
    "def predict_matching_matrix(model, data1, data2, use_hungarian: bool = True):\n",
    "    \"\"\"\n",
    "    Produces a matching matrix between data1 and data2.\n",
    "    If use_hungarian=True, applies the Hungarian algorithm to the similarity scores.\n",
    "    Otherwise returns the raw similarity matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        batch_idx1 = torch.zeros(data1.num_nodes, dtype=torch.long, device=device)\n",
    "        batch_idx2 = torch.zeros(data2.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "        sim_matrix_list, _ = model(data1, data2, batch_idx1, batch_idx2, inference=True)\n",
    "        sim = sim_matrix_list[0].unsqueeze(0)  # [1, N1, N2]\n",
    "\n",
    "        n1 = torch.tensor([sim.shape[1]], dtype=torch.int32, device=device)\n",
    "        n2 = torch.tensor([sim.shape[2]], dtype=torch.int32, device=device)\n",
    "\n",
    "        if use_hungarian:\n",
    "            # returns a hard assignment matrix [N1, N2]\n",
    "            return pygmtools.hungarian(sim, n1=n1, n2=n2).squeeze(0)\n",
    "        else:\n",
    "            # return soft scores [N1, N2]\n",
    "            return sim.squeeze(0)\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, train_loader, val_loader, num_epochs, writer,\n",
    "               best_model_path='checkpoint.pt', final_model_path='final_model.pt',\n",
    "               patience=10, resume=False):\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    patience_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_embeddings_history = []\n",
    "\n",
    "    # Resume from checkpoint if requested\n",
    "    if resume and os.path.exists(best_model_path):\n",
    "        print(f\"Loading checkpoint from {best_model_path}\")\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_epoch = checkpoint['best_epoch']\n",
    "        print(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            # Train\n",
    "            train_loss, _ = train_epoch_sinkhorn(model, train_loader, optimizer, writer, epoch)\n",
    "            # Evaluate\n",
    "            val_acc, val_loss, val_embeddings = evaluate_sinkhorn(model, val_loader)\n",
    "            \n",
    "            log_metrics(writer, {\"loss\": train_loss}, epoch, prefix=\"train\")\n",
    "            log_metrics(writer, {\"loss\": val_loss, \"acc\": val_acc}, epoch, prefix=\"val\")\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_embeddings_history.append(val_embeddings)\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'best_epoch': best_epoch\n",
    "                }, best_model_path)\n",
    "                print(f\"[Epoch {epoch}] Saved new best model.\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            print(f\"Epoch {epoch:03} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. Best was {best_epoch}.\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted manually (Ctrl+C).\")\n",
    "\n",
    "    log_metrics(writer, {\"best_val_loss\": best_val_loss, \"best_epoch\": best_epoch}, epoch, prefix=\"best\")\n",
    "    writer.close()\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_epoch': best_epoch\n",
    "    }, final_model_path)\n",
    "    print(\"Final model saved.\")\n",
    "\n",
    "    return train_losses, val_losses, val_embeddings_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            MODELS\n",
    "#----------------------------------------\n",
    "\n",
    "# SG-pgm model adaptation\n",
    "# class MatchingModel_GATv2SinkhornTopK(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_dim: int,\n",
    "#         hidden_dim: int,\n",
    "#         out_dim: int,\n",
    "#         sinkhorn_max_iter: int = 20,\n",
    "#         sinkhorn_tau: float = 5e-2,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # ─── 1) GNN backbone: two-layer GATv2\n",
    "#         # First GATv2Conv projects in_dim → hidden_dim, apply ReLU\n",
    "#         # Second GATv2Conv projects hidden_dim → out_dim, no activation afterwards\n",
    "#         self.gnn = nn.ModuleList([\n",
    "#             GATv2Conv(in_dim, hidden_dim),\n",
    "#             GATv2Conv(hidden_dim, out_dim),\n",
    "#         ])\n",
    "#         # InstanceNorm to normalize each [N1×N2] similarity map\n",
    "#         self.inst_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "\n",
    "#         # ─── 2) AFA-U “unified” module to predict number of inliers K\n",
    "#         #  univ_size = maximum graph size, used to pad all embeddings to fixed length\n",
    "#         self.k_top_encoder = AFAUEncoder()\n",
    "\n",
    "#         # Two small MLPs to reduce pooled embedding → scalar in [0,1]\n",
    "#         self.final_row = nn.Sequential(\n",
    "#             nn.Linear(out_dim, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(8, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#         self.final_col = nn.Sequential(\n",
    "#             nn.Linear(out_dim, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(8, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         # Sinkhorn-TopK hyperparams\n",
    "#         self.sinkhorn_max_iter = sinkhorn_max_iter\n",
    "#         self.sinkhorn_tau      = sinkhorn_tau\n",
    "\n",
    "#     def encode(self, x, edge_index):\n",
    "#         \"\"\"\n",
    "#         Pass input features x through the two GATv2Conv layers.\n",
    "#         Apply ReLU after the first, but not after the last.\n",
    "#         \"\"\"\n",
    "#         for i, conv in enumerate(self.gnn):\n",
    "#             x = conv(x, edge_index)\n",
    "#             if i < len(self.gnn) - 1:\n",
    "#                 x = F.relu(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, batch1, batch2, batch_idx1=None, batch_idx2=None):\n",
    "#         \"\"\"\n",
    "#         batch1, batch2: PyG Data objects for the two graphs in each pair.\n",
    "#         batch_idx1, batch_idx2: optional precomputed batch assignments.\n",
    "#         Returns a list of final hard match matrices (perm_pred_list) and\n",
    "#         the raw embeddings for each graph pair (all_embeddings).\n",
    "#         \"\"\"\n",
    "#         # device = next(self.parameters()).device\n",
    "\n",
    "#         # ─── 1) Unpack node features & edge indices, move to GPU/CPU\n",
    "#         x1, edge1 = batch1.x.to(device), batch1.edge_index.to(device)\n",
    "#         x2, edge2 = batch2.x.to(device), batch2.edge_index.to(device)\n",
    "\n",
    "#         # ─── 2) Determine which node belongs to which graph in the batch\n",
    "#         #    If not supplied, read from Data.batch\n",
    "#         batch_idx1 = batch1.batch.to(device) if batch_idx1 is None else batch_idx1.to(device)\n",
    "#         batch_idx2 = batch2.batch.to(device) if batch_idx2 is None else batch_idx2.to(device)\n",
    "\n",
    "#         # ─── 3) Encode both sets of nodes via the GNN\n",
    "#         h1 = self.encode(x1, edge1)  # [total_nodes1, out_dim]\n",
    "#         h2 = self.encode(x2, edge2)  # [total_nodes2, out_dim]\n",
    "\n",
    "#         # How many graph pairs in this minibatch?\n",
    "#         B = batch_idx1.max().item() + 1\n",
    "\n",
    "#         perm_pred_list = []\n",
    "#         all_embeddings = []\n",
    "\n",
    "#         for b in range(B):\n",
    "#             # Isolate embeddings for the b-th graph pair\n",
    "#             h1_i = h1[batch_idx1 == b]  # shape [N1, d]\n",
    "#             h2_i = h2[batch_idx2 == b]  # shape [N2, d]\n",
    "#             N1, N2 = h1_i.size(0), h2_i.size(0)\n",
    "\n",
    "#             # ─── 4) Compute raw similarity: dot product between all node pairs\n",
    "#             sim = torch.matmul(h1_i, h2_i.T)    # [N1, N2]\n",
    "#             # Normalize per-instance so Sinkhorn is stable\n",
    "#             sim_b = sim.unsqueeze(0).unsqueeze(1)   # [1,1,N1,N2]\n",
    "#             sim_n = self.inst_norm(sim_b).squeeze(1)  # [1,N1,N2]\n",
    "\n",
    "#             # Prepare row/col sizes for pygmtools\n",
    "#             n1_t = torch.tensor([N1], dtype=torch.int32, device=device)\n",
    "#             n2_t = torch.tensor([N2], dtype=torch.int32, device=device)\n",
    "\n",
    "#             # Soft Sinkhorn → soft_match [N1,N2]\n",
    "#             soft_S = pygmtools.sinkhorn(sim_n, n1=n1_t, n2=n2_t, dummy_row=False)[0]\n",
    "\n",
    "#             # ─── 5) AFA-U predicts inlier count K from soft matching\n",
    "#             #   a) Expand dims to batch form\n",
    "#             row_emb = h1_i.unsqueeze(0)      # [1, N1, d]\n",
    "#             col_emb = h2_i.unsqueeze(0)      # [1, N2, d]\n",
    "#             cost_mat = sim_n                 # [1, N1, N2]\n",
    "\n",
    "#             #   b) Run the bipartite-attention encoder\n",
    "#             out_r, out_c = self.k_top_encoder(row_emb, col_emb, cost_mat) # [1, N1, d], [1, N2, d]\n",
    "            \n",
    "#             #   c) Dynamic max over nodes\n",
    "#             g_r = out_r.max(dim=1).values     # [1, d]\n",
    "#             g_c = out_c.max(dim=1).values     # [1, d]\n",
    "\n",
    "#             #   d) Small MLPs → fraction in [0,1]\n",
    "#             k_r = self.final_row(g_r).squeeze(-1)  # [1]\n",
    "#             k_c = self.final_col(g_c).squeeze(-1)  # [1]\n",
    "#             ks  = (k_r + k_c) / 2                  # [1] average of row/col predictions\n",
    "\n",
    "#             # ─── 6) Top-K matching\n",
    "\n",
    "#             if self.training:\n",
    "#                 # use ground-truth K\n",
    "#                 ks_gt = torch.tensor([N2], dtype=torch.long, device=device)\n",
    "#                 hard_S, soft_S = soft_topk(\n",
    "#                     sim_n, ks_gt,\n",
    "#                     max_iter=self.sinkhorn_max_iter,\n",
    "#                     tau=self.sinkhorn_tau,\n",
    "#                     nrows=n1_t, ncols=n2_t,\n",
    "#                     return_prob=True\n",
    "#                 )\n",
    "#                 perm_pred_list.append(soft_S[0])\n",
    "#             else:\n",
    "#                 ks_eff = (ks * N2).long()\n",
    "#                 hard_S = soft_topk(\n",
    "#                     sim_n, ks_eff,\n",
    "#                     max_iter=self.sinkhorn_max_iter,\n",
    "#                     tau=self.sinkhorn_tau,\n",
    "#                     nrows=n1_t, ncols=n2_t,\n",
    "#                     return_prob=False\n",
    "#                 )\n",
    "#                 perm_pred_list.append(hard_S[0])\n",
    "\n",
    "\n",
    "#             # ─── 7) Collect outputs for this pair\n",
    "#             all_embeddings.append((h1_i, h2_i))   # store embeddings for any downstream use\n",
    "\n",
    "#         return perm_pred_list, all_embeddings\n",
    "\n",
    "# Iperparametri\n",
    "\n",
    "in_dim = 7\n",
    "hidden_dim = 64\n",
    "out_dim = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "weight_decay = 5e-5\n",
    "patience = 30\n",
    "\n",
    "###     GRAPH MATCHING MODEL\n",
    "class MatchingModel_GATv2Sinkhorn(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, attention_dropout=0.1, dropout_emb=0.1, temperature: float = 1.0, max_iter: int = 10, tau: float = 1):\n",
    "        super().__init__()\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GATv2Conv(in_dim, hidden_dim, dropout=attention_dropout),\n",
    "            GATv2Conv(hidden_dim, out_dim, dropout=attention_dropout)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(p=dropout_emb)\n",
    "        # # bilinear weight matrix A per affinity\n",
    "        # std = 1.0 / math.sqrt(out_dim)\n",
    "        # self.A = nn.Parameter(torch.randn(out_dim, out_dim) * std)\n",
    "        # self.temperature = temperature\n",
    "        # InstanceNorm per-sample\n",
    "        self.inst_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "        # Sinkhorn hyperparams\n",
    "        self.max_iter = max_iter\n",
    "        self.tau = tau\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.gnn):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.gnn) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, batch1, batch2, batch_idx1=None, batch_idx2=None, inference=False):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        x1, edge1 = batch1.x.to(device), batch1.edge_index.to(device)\n",
    "        x2, edge2 = batch2.x.to(device), batch2.edge_index.to(device)\n",
    "\n",
    "        batch_idx1 = batch1.batch.to(device) if batch_idx1 is None else batch_idx1.to(device)\n",
    "        batch_idx2 = batch2.batch.to(device) if batch_idx2 is None else batch_idx2.to(device)\n",
    "\n",
    "        h1 = self.encode(x1, edge1)\n",
    "        h2 = self.encode(x2, edge2)\n",
    "\n",
    "        B = batch_idx1.max().item() + 1\n",
    "        perm_pred_list = []\n",
    "        all_embeddings = []\n",
    "\n",
    "        for b in range(B):\n",
    "            h1_b = h1[batch_idx1 == b]   # [n1, d]\n",
    "            h2_b = h2[batch_idx2 == b]   # [n2, d]\n",
    "\n",
    "            # # ---- bilinear affinity ----\n",
    "            # # scores_{ij} = (h1_b @ A @ h2_b.T) / temperature\n",
    "            # scores = (h1_b @ self.A) @ h2_b.T\n",
    "            # M = torch.exp(scores / self.temperature)  # [n1, n2]\n",
    "            # # normalize and sinkhorn\n",
    "            # M_batched = M.unsqueeze(0).unsqueeze(1)  # [1,1,n1,n2]\n",
    "            # M_normed = self.inst_norm(M_batched).squeeze(1)  # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "            # affinity matrix + normalization + sinkhorn\n",
    "            sim = torch.matmul(h1_b, h2_b.T) # [n1, n2]\n",
    "            sim_batched = sim.unsqueeze(0).unsqueeze(1) # [1,1,n1,n2]\n",
    "            sim_normed = self.inst_norm(sim_batched).squeeze(1) # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "            S = pygmtools.sinkhorn(sim_normed, max_iter=self.max_iter, tau=self.tau)[0]\n",
    "            perm_pred_list.append(S)\n",
    "            all_embeddings.append((h1_b, h2_b))\n",
    "\n",
    "        return perm_pred_list, all_embeddings\n",
    "\n",
    "###     PARTIAL GRAPH MATCHING MODEL\n",
    "class MatchingModel_GATv2SinkhornTopK(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        sinkhorn_max_iter: int = 15,\n",
    "        sinkhorn_tau: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        dropout_emb: float = 0.1,\n",
    "        temperature: float = 1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GATv2Conv(in_dim, hidden_dim, dropout=attention_dropout),\n",
    "            GATv2Conv(hidden_dim, out_dim, dropout=attention_dropout),\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(p=dropout_emb)\n",
    "        # # bilinear weight matrix A per affinity\n",
    "        # std = 1.0 / math.sqrt(out_dim)\n",
    "        # self.A = nn.Parameter(torch.randn(out_dim, out_dim) * std)\n",
    "        # # temperature per affinities\n",
    "        # self.temperature = temperature\n",
    "        # InstanceNorm per-sample\n",
    "        self.inst_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "        # Sinkhorn hyperparams\n",
    "        self.sinkhorn_max_iter = sinkhorn_max_iter\n",
    "        self.sinkhorn_tau = sinkhorn_tau\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.gnn):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.gnn) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, batch1, batch2, batch_idx1=None, batch_idx2=None, inference=False):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        x1, edge1 = batch1.x.to(device), batch1.edge_index.to(device)\n",
    "        x2, edge2 = batch2.x.to(device), batch2.edge_index.to(device)\n",
    "\n",
    "        batch_idx1 = batch1.batch.to(device) if batch_idx1 is None else batch_idx1.to(device)\n",
    "        batch_idx2 = batch2.batch.to(device) if batch_idx2 is None else batch_idx2.to(device)\n",
    "\n",
    "        h1 = self.encode(x1, edge1)\n",
    "        h2 = self.encode(x2, edge2)\n",
    "\n",
    "        B = batch_idx1.max().item() + 1\n",
    "        perm_pred_list = []\n",
    "        all_embeddings = []\n",
    "\n",
    "        for b in range(B):\n",
    "            h1_b = h1[batch_idx1 == b]   # [n1, d]\n",
    "            h2_b = h2[batch_idx2 == b]   # [n2, d]\n",
    "            N1, N2 = h1_b.size(0), h2_b.size(0)\n",
    "\n",
    "            # # ---- bilinear affinity ----\n",
    "            # # scores_{ij} = (h1_b @ A @ h2_b.T) / temperature\n",
    "            # scores = (h1_b @ self.A) @ h2_b.T\n",
    "            # M = torch.exp(scores / self.temperature)  # [n1, n2]\n",
    "            # # normalize and sinkhorn\n",
    "            # M_batched = M.unsqueeze(0).unsqueeze(1)  # [1,1,n1,n2]\n",
    "            # M_normed = self.inst_norm(M_batched).squeeze(1)  # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "            # affinity matrix + normalization + sinkhorn\n",
    "            sim = torch.matmul(h1_b, h2_b.T) # [n1, n2]\n",
    "            sim_batched = sim.unsqueeze(0).unsqueeze(1) # [1,1,n1,n2]\n",
    "            sim_normed = self.inst_norm(sim_batched).squeeze(1) # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "            n1_t = torch.tensor([N1], dtype=torch.int32, device=device)\n",
    "            n2_t = torch.tensor([N2], dtype=torch.int32, device=device)\n",
    "            S = pygmtools.sinkhorn(sim_normed, n1=n1_t, n2=n2_t, max_iter=self.sinkhorn_max_iter, tau=self.sinkhorn_tau)\n",
    "            \n",
    "            ks_gt = torch.tensor([N2], dtype=torch.long, device=device)\n",
    "            \n",
    "            hard_S, soft_S = soft_topk(\n",
    "                S, ks_gt,\n",
    "                max_iter=self.sinkhorn_max_iter,\n",
    "                tau=self.sinkhorn_tau,\n",
    "                nrows=n1_t, ncols=n2_t,\n",
    "                return_prob=True\n",
    "            )\n",
    "\n",
    "            if inference:\n",
    "                perm_pred_list.append(hard_S[0])\n",
    "            else:\n",
    "                perm_pred_list.append(soft_S[0])\n",
    "\n",
    "            all_embeddings.append((h1_b, h2_b))\n",
    "\n",
    "        return perm_pred_list, all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            PARAMETERS OPTIMIZATION\n",
    "#----------------------------------------\n",
    "\n",
    "def objective_gm(trial, train_dataset, val_dataset, path):\n",
    "    # iperparametri da esplorare\n",
    "    lr           = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    dropout      = trial.suggest_uniform(\"dropout\", 0.0, 0.6)\n",
    "    hidden_dim   = trial.suggest_categorical(\"hidden_dim\", [32, 64, 128])\n",
    "    out_dim      = trial.suggest_categorical(\"out_dim\",    [16, 32, 64])\n",
    "    batch_size   = trial.suggest_categorical(\"batch_size\", [2, 4, 8])\n",
    "    heads        = trial.suggest_int(\"heads\",           1,   4)\n",
    "    attn_dropout = trial.suggest_uniform(\"attn_dropout\", 0.0, 0.6)\n",
    "    num_layers   = trial.suggest_int(\"num_layers\",       1,   3)\n",
    "    sinkhorn_tau = trial.suggest_loguniform(\"tau\",      1e-3, 1e-1)\n",
    "    max_iter     = trial.suggest_int(\"max_iter\",        10, 100)\n",
    "\n",
    "    # dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "    # modello\n",
    "    class MatchingModel(nn.Module):\n",
    "        def __init__(self,dropout, hidden_dim, out_dim, heads, attn_dropout, num_layers, sinkhorn_tau, max_iter):\n",
    "            super().__init__()\n",
    "            self.gnn = nn.ModuleList()\n",
    "            dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "            for i in range(num_layers):\n",
    "                # new: always average the heads so the feature‐dim stays dims[i+1]\n",
    "                self.gnn.append(\n",
    "                GATv2Conv(dims[i], dims[i+1],\n",
    "                            heads=heads, concat=False,\n",
    "                            dropout=attn_dropout))\n",
    "            self.dropout = dropout\n",
    "            # # bilinear weight matrix A per affinity\n",
    "            # std = 1.0 / math.sqrt(out_dim)\n",
    "            # self.A = nn.Parameter(torch.randn(out_dim, out_dim) * std)\n",
    "            # self.temperature = temperature\n",
    "            # InstanceNorm per-sample\n",
    "            self.inst_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "            self.tau = sinkhorn_tau\n",
    "            self.max_iter = max_iter\n",
    "\n",
    "        def encode(self, x, edge_index):\n",
    "            for i, conv in enumerate(self.gnn):\n",
    "                x = conv(x, edge_index)\n",
    "                if i < len(self.gnn)-1:\n",
    "                    x = F.relu(x)\n",
    "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            return x\n",
    "\n",
    "        def forward(self, batch1, batch2, perm_list, batch_idx1=None, batch_idx2=None, inference=False):\n",
    "            device = next(self.parameters()).device\n",
    "            x1, e1 = batch1.x.to(device), batch1.edge_index.to(device)\n",
    "            x2, e2 = batch2.x.to(device), batch2.edge_index.to(device)\n",
    "            perm_list = [p.to(device) for p in perm_list]\n",
    "            \n",
    "            if batch_idx1 is None:\n",
    "                batch_idx1 = batch1.batch.to(device)\n",
    "                batch_idx2 = batch2.batch.to(device)\n",
    "            h1 = self.encode(x1, e1)\n",
    "            h2 = self.encode(x2, e2)\n",
    "            B = batch_idx1.max().item()+1\n",
    "            loss = 0\n",
    "            for b in range(B):\n",
    "                h1_b = h1[batch_idx1==b]\n",
    "                h2_b = h2[batch_idx2==b]\n",
    "\n",
    "                # affinity matrix + normalization + sinkhorn\n",
    "                sim = torch.matmul(h1_b, h2_b.T) # [n1, n2]\n",
    "                sim_batched = sim.unsqueeze(0).unsqueeze(1) # [1,1,n1,n2]\n",
    "                sim_normed = self.inst_norm(sim_batched).squeeze(1) # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "                S = pygmtools.sinkhorn(sim_normed, tau=self.tau, max_iter=self.max_iter)[0]\n",
    "                loss = loss + bce_permutation_loss(S, perm_list[b])\n",
    "            return loss / B\n",
    "\n",
    "    model = MatchingModel(\n",
    "        dropout=dropout,\n",
    "        hidden_dim=hidden_dim,\n",
    "        out_dim=out_dim,\n",
    "        heads=heads,\n",
    "        attn_dropout=attn_dropout,\n",
    "        num_layers=num_layers,\n",
    "        sinkhorn_tau=sinkhorn_tau,\n",
    "        max_iter=max_iter\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # training rapido con early stopping\n",
    "    best_val = float('inf')\n",
    "    counter = 0\n",
    "    for epoch in range(30):\n",
    "        # train\n",
    "        model.train()\n",
    "        for b1, b2, perm in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(b1, b2, perm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for b1, b2, perm in val_loader:\n",
    "                val_loss += model(b1, b2, perm).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= 5:\n",
    "                break\n",
    "    \n",
    "    # save best trial info\n",
    "    if trial.number == 0 or best_val <= trial.study.best_value:\n",
    "        result = {\n",
    "            \"val_loss\": best_val,\n",
    "            \"params\": trial.params\n",
    "        }\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(os.path.join(path, \"best_trial_results.json\"), \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "def objective_pgm(trial, train_dataset, val_dataset, path):\n",
    "    lr           = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    dropout_emb  = trial.suggest_uniform(\"dropout\", 0.0, 0.6)\n",
    "    hidden_dim   = trial.suggest_categorical(\"hidden_dim\", [32, 64, 128])\n",
    "    out_dim      = trial.suggest_categorical(\"out_dim\",    [16, 32, 64])\n",
    "    batch_size   = trial.suggest_categorical(\"batch_size\", [2, 4, 8])\n",
    "    attn_dropout = trial.suggest_uniform(\"attn_dropout\", 0.0, 0.6)\n",
    "    max_iter     = trial.suggest_int(\"max_iter\",        10, 100)\n",
    "    tau          = trial.suggest_loguniform(\"tau\",      1e-3, 1e-1)\n",
    "    num_layers   = trial.suggest_int(\"num_layers\",       1, 3)\n",
    "    heads        = trial.suggest_int(\"heads\",           1,   4)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "    # Flexible model for partial matching\n",
    "    class MatchingModel_GATv2SinkhornTopK_OPT(nn.Module):\n",
    "        def __init__(self, in_dim, hidden_dim, out_dim, sinkhorn_max_iter, sinkhorn_tau,\n",
    "                    attention_dropout, dropout_emb, num_layers, heads):\n",
    "            super().__init__()\n",
    "            self.gnn = nn.ModuleList()\n",
    "            dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "            for i in range(num_layers):\n",
    "                # new: always average the heads so the feature‐dim stays dims[i+1]\n",
    "                self.gnn.append(\n",
    "                GATv2Conv(dims[i], dims[i+1],\n",
    "                            heads=heads, concat=False,\n",
    "                            dropout=attention_dropout))\n",
    "            self.dropout = nn.Dropout(p=dropout_emb)\n",
    "            # # bilinear weight matrix A per affinity\n",
    "            # std = 1.0 / math.sqrt(out_dim)\n",
    "            # self.A = nn.Parameter(torch.randn(out_dim, out_dim) * std)\n",
    "            # self.temperature = temperature\n",
    "            # InstanceNorm per-sample\n",
    "            self.inst_norm = nn.InstanceNorm2d(1, affine=True)\n",
    "            self.sinkhorn_max_iter = sinkhorn_max_iter\n",
    "            self.sinkhorn_tau = sinkhorn_tau\n",
    "\n",
    "        def encode(self, x, edge_index):\n",
    "            for i, conv in enumerate(self.gnn):\n",
    "                x = conv(x, edge_index)\n",
    "                if i < len(self.gnn) - 1:\n",
    "                    x = F.relu(x)\n",
    "                    x = self.dropout(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, batch1, batch2, perm_list, batch_idx1=None, batch_idx2=None, inference=False):\n",
    "            device = next(self.parameters()).device\n",
    "            x1, edge1 = batch1.x.to(device), batch1.edge_index.to(device)\n",
    "            x2, edge2 = batch2.x.to(device), batch2.edge_index.to(device)\n",
    "            perm_list = [p.to(device) for p in perm_list]\n",
    "\n",
    "            batch_idx1 = batch1.batch.to(device) if batch_idx1 is None else batch_idx1.to(device)\n",
    "            batch_idx2 = batch2.batch.to(device) if batch_idx2 is None else batch_idx2.to(device)\n",
    "\n",
    "            h1 = self.encode(x1, edge1)\n",
    "            h2 = self.encode(x2, edge2)\n",
    "\n",
    "            B = batch_idx1.max().item() + 1\n",
    "            loss = 0.0\n",
    "            for b in range(B):\n",
    "                h1_b = h1[batch_idx1 == b]\n",
    "                h2_b = h2[batch_idx2 == b]\n",
    "\n",
    "                # affinity matrix + normalization + sinkhorn\n",
    "                sim = torch.matmul(h1_b, h2_b.T) # [n1, n2]\n",
    "                sim_batched = sim.unsqueeze(0).unsqueeze(1) # [1,1,n1,n2]\n",
    "                sim_normed = self.inst_norm(sim_batched).squeeze(1) # [1,n1,n2] -> [n1,n2]\n",
    "\n",
    "                n1 = torch.tensor([h1_b.size(0)], dtype=torch.int32, device=device)\n",
    "                n2 = torch.tensor([h2_b.size(0)], dtype=torch.int32, device=device)\n",
    "                S = pygmtools.sinkhorn(sim_normed, n1=n1, n2=n2, max_iter=self.sinkhorn_max_iter, tau=self.sinkhorn_tau)\n",
    "\n",
    "                ks_gt = torch.tensor([h2_b.size(0)], dtype=torch.long, device=device)\n",
    "\n",
    "                _, soft_S = soft_topk(S, ks_gt, max_iter=self.sinkhorn_max_iter,\n",
    "                                    tau=self.sinkhorn_tau, nrows=n1, ncols=n2,\n",
    "                                    return_prob=True)\n",
    "\n",
    "                loss += bce_permutation_loss(soft_S[0], perm_list[b])\n",
    "            return loss / B\n",
    "\n",
    "    model = MatchingModel_GATv2SinkhornTopK_OPT(\n",
    "        in_dim=train_dataset[0][0].x.size(1),\n",
    "        hidden_dim=hidden_dim,\n",
    "        out_dim=out_dim,\n",
    "        sinkhorn_max_iter=max_iter,\n",
    "        sinkhorn_tau=tau,\n",
    "        attention_dropout=attn_dropout,\n",
    "        dropout_emb=dropout_emb,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    counter = 0\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        for b1, b2, perm in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(b1, b2, perm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for b1, b2, perm in val_loader:\n",
    "                val_loss += model(b1, b2, perm).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= 5:\n",
    "                break\n",
    "\n",
    "    # save best trial info\n",
    "    if trial.number == 0 or best_val <= trial.study.best_value:\n",
    "        result = {\n",
    "            \"val_loss\": best_val,\n",
    "            \"params\": trial.params\n",
    "        }\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(os.path.join(path, \"best_trial_results.json\"), \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#            XAI UTILS\n",
    "#----------------------------------------\n",
    "def plt2arr(fig):\n",
    "    \"\"\"\n",
    "    Converts a matplotlib figure to a NumPy RGB array.\n",
    "    Ensures the canvas is drawn before reading pixel data.\n",
    "    \"\"\"\n",
    "    # Attach a canvas if not already present\n",
    "    if fig.canvas is None or not isinstance(fig.canvas, FigureCanvas):\n",
    "        FigureCanvas(fig)\n",
    "\n",
    "    # Force the figure to render\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Get the image from the buffer\n",
    "    buf = fig.canvas.buffer_rgba()\n",
    "    img = np.asarray(buf)\n",
    "\n",
    "    # Remove the alpha channel (RGBA -> RGB)\n",
    "    img_rgb = img[..., :3].copy()\n",
    "\n",
    "    return img_rgb\n",
    "\n",
    "def get_node_type_labels(h):\n",
    "    if h.shape[1] < 2:\n",
    "        raise ValueError(\"The embedding must have at least two dimensions to distinguish types.\")\n",
    "    return torch.argmax(h[:, :2], dim=1)  # 0 = room, 1 = ws\n",
    "\n",
    "def visualize(h, node_type_labels, graph_labels, epoch, node_type_filter: Optional[Literal[\"room\", \"ws\", \"all\"]] = \"all\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), frameon=False)\n",
    "    fig.suptitle(f'Epoch index = {epoch}')\n",
    "    z = TSNE(2, random_state=42, init='pca').fit_transform(h.cpu().numpy())\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    appearance = {\n",
    "        (0, 0): ('tab:blue', 'o'),    # G1 - room\n",
    "        (0, 1): ('tab:green', 's'),   # G1 - ws\n",
    "        (1, 0): ('tab:orange', '^'),  # G2 - room\n",
    "        (1, 1): ('tab:red', 'D'),     # G2 - ws\n",
    "    }\n",
    "\n",
    "    for g in [0, 1]:\n",
    "        for t in [0, 1]:\n",
    "            if node_type_filter == \"room\" and t != 0:\n",
    "                continue\n",
    "            if node_type_filter == \"ws\" and t != 1:\n",
    "                continue\n",
    "            mask = ((graph_labels == g) & (node_type_labels == t)).cpu().numpy()\n",
    "            if np.any(mask):\n",
    "                z_sub = z[mask]\n",
    "                color, marker = appearance[(g, t)]\n",
    "                label = f\"{'G1' if g==0 else 'G2'} - {'room' if t==0 else 'ws'}\"\n",
    "                ax.scatter(z_sub[:, 0], z_sub[:, 1],\n",
    "                           s=50, c=color, marker=marker,\n",
    "                           edgecolors='k', linewidths=0.5,\n",
    "                           alpha=0.4, label=label)\n",
    "\n",
    "    ax.legend(loc='best', frameon=True)\n",
    "    fig.canvas.draw()\n",
    "    arr = plt2arr(fig)\n",
    "    plt.close(fig)\n",
    "    return arr\n",
    "\n",
    "def create_embedding_gif_stride(history, output_path, embedding_type, pair=0, fps=1, step=5, node_type_filter: Optional[Literal[\"room\", \"ws\", \"all\"]] = \"all\"):\n",
    "    plt.close('all')\n",
    "    images = []\n",
    "\n",
    "    for epoch in range(0, len(history), step):\n",
    "        h1, h2 = history[epoch][pair]\n",
    "        h = torch.cat([h1, h2], dim=0)\n",
    "\n",
    "        node_types = embedding_type\n",
    "        graph_labels = torch.cat([\n",
    "            torch.zeros(h1.size(0), dtype=torch.long),\n",
    "            torch.ones(h2.size(0), dtype=torch.long)\n",
    "        ], dim=0)\n",
    "\n",
    "        graph_labels = graph_labels.to(device)\n",
    "        node_types = node_types.to(device)\n",
    "        \n",
    "        images.append(visualize(h, node_types, graph_labels, epoch, node_type_filter=node_type_filter))\n",
    "\n",
    "    clip = ImageSequenceClip(images, fps=fps)\n",
    "    clip.write_gif(output_path, fps=fps)\n",
    "    print(f\"GIF saved at: {output_path}\")\n",
    "\n",
    "def visualize_initial_embeddings(h1, h2, output_path, node_type_filter: Optional[Literal[\"room\", \"ws\", \"all\"]] = \"all\"):\n",
    "    h = torch.cat([h1, h2], dim=0)\n",
    "    node_types = get_node_type_labels(h)\n",
    "    graph_labels = torch.cat([\n",
    "        torch.zeros(h1.size(0), dtype=torch.long),\n",
    "        torch.ones(h2.size(0), dtype=torch.long)\n",
    "    ], dim=0)\n",
    "\n",
    "    graph_labels = graph_labels.to(device)\n",
    "    node_types = node_types.to(device)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), frameon=False)\n",
    "    z = TSNE(2, random_state=42, init='pca').fit_transform(h.cpu().numpy())\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    appearance = {\n",
    "        (0, 0): ('tab:blue', 'o'),\n",
    "        (0, 1): ('tab:green', 's'),\n",
    "        (1, 0): ('tab:orange', '^'),\n",
    "        (1, 1): ('tab:red', 'D'),\n",
    "    }\n",
    "\n",
    "    for g in [0, 1]:\n",
    "        for t in [0, 1]:\n",
    "            if node_type_filter == \"room\" and t != 0:\n",
    "                continue\n",
    "            if node_type_filter == \"ws\" and t != 1:\n",
    "                continue\n",
    "            mask = ((graph_labels == g) & (node_types == t)).cpu().numpy()\n",
    "            if np.any(mask):\n",
    "                z_sub = z[mask]\n",
    "                color, marker = appearance[(g, t)]\n",
    "                label = f\"{'G1' if g==0 else 'G2'} - {'room' if t==0 else 'ws'}\"\n",
    "                ax.scatter(z_sub[:, 0], z_sub[:, 1],\n",
    "                           s=50, c=color, marker=marker,\n",
    "                           edgecolors='k', linewidths=0.5,\n",
    "                           alpha=0.4, label=label)\n",
    "\n",
    "    ax.legend(loc='best', frameon=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return node_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: generates a DiGraph with nodes A0…A4, random attributes, and p_edge as edge probability\n",
    "def make_random_graph(name, n_nodes=5, p_edge=0.4):\n",
    "    G = nx.DiGraph(name=name)\n",
    "    for i in range(n_nodes):\n",
    "        nid = f'N{i}'\n",
    "        # random type, center uniformly in [0,10], normal random unit vector, random length\n",
    "        t = random.choice(['room','ws'])\n",
    "        center = [random.uniform(0,10), random.uniform(0,10)]\n",
    "        v = torch.randn(2).tolist()\n",
    "        norm = [v[0]/(abs(v[0])+abs(v[1])+1e-6), v[1]/(abs(v[0])+abs(v[1])+1e-6)]\n",
    "        length = random.uniform(1,5)\n",
    "        G.add_node(nid, type=t, center=center, normal=norm, length=length)\n",
    "    # adds edges with probability p_edge\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u!=v and random.random() < p_edge:\n",
    "                G.add_edge(u, v, weight=random.uniform(0.1,1.0))\n",
    "    return G\n",
    "\n",
    "# 1) Generate 6 graphs\n",
    "graphs = [make_random_graph(f'G{i}') for i in range(6)]\n",
    "print(\"Total graphs generated:\", len(graphs))\n",
    "\n",
    "# 3) Test full matching (g0 vs g0 itself)\n",
    "pairs_full = []\n",
    "generate_matching_pair_as_data(graphs[0], graphs[0], pairs_full)\n",
    "\n",
    "# # 3.1) Split 70/15/15 \n",
    "# train, val, test = split_graphs_stratified(pairs_full, train_frac=0.6, val_frac=0.2, test_frac=0.2)\n",
    "# print(\"Split sizes → train:\", len(train), \"val:\", len(val), \"test:\", len(test))\n",
    "# assert len(train)==4 and len(val)==1 and len(test)==1, \"Split not conforming\"\n",
    "\n",
    "pyg1_f, pyg2_f, P_full = pairs_full[0]\n",
    "n = len(graphs[0].nodes())\n",
    "print(\"Full match shape:\", P_full.shape)\n",
    "assert P_full.shape == (n, n)\n",
    "assert torch.allclose(P_full.sum(dim=0), torch.ones(n))\n",
    "assert torch.allclose(P_full.sum(dim=1), torch.ones(n))\n",
    "print(\"Full matching OK\")\n",
    "\n",
    "# 4) Test partial matching (remove 2 nodes from G1)\n",
    "g_partial = graphs[1].copy()\n",
    "to_remove = random.sample(list(g_partial.nodes()), 2)\n",
    "for u in to_remove:\n",
    "    g_partial.remove_node(u)\n",
    "pairs_part = []\n",
    "generate_matching_pair_as_data(graphs[1], g_partial, pairs_part)\n",
    "pyg1_p, pyg2_p, P_part = pairs_part[0]\n",
    "print(\"Partial match shape:\", P_part.shape, \n",
    "      f\"(original {len(graphs[1].nodes())} vs partial {len(g_partial.nodes())})\")\n",
    "# each column max 1, each row max 1\n",
    "assert (P_part.sum(dim=0) <= 1).all()\n",
    "assert (P_part.sum(dim=1) <= 1).all()\n",
    "print(\"Partial matching OK\\n\", P_part)\n",
    "\n",
    "# 5) Test round-trip Data→NX on one of the matches\n",
    "recon = pyg_data_to_nx_digraph(pyg2_p, [graphs[1], g_partial])\n",
    "assert set(recon.nodes()) == set(g_partial.nodes())\n",
    "print(\"Round-trip PyG→NX OK\")\n",
    "\n",
    "# plot_two_graphs_with_matching(\n",
    "#     graphs_list=[pyg1_p, pyg2_p],\n",
    "#     gt_perm=P_part,\n",
    "#     original_graphs=[graphs[1]],\n",
    "#     noise_graphs=[g_partial],\n",
    "#     pred_perm=P_part,\n",
    "#     match_display=\"all\"\n",
    "# )\n",
    "\n",
    "print(\"All advanced tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RCatL5UdsZV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBpjTdWOdsZV"
   },
   "outputs": [],
   "source": [
    "# Construct the folders if they don't exist\n",
    "# GNN_PATH\n",
    "# ├── models\n",
    "# │   ├── graph matching\n",
    "# │   │   ├── equal\n",
    "# │   │   ├── global\n",
    "# │   │   ├── global_local\n",
    "# │   │   └── local\n",
    "# │   └── partial graph matching\n",
    "# │       ├── room_dropout_equal\n",
    "# │       ├── ws_dropout_equal\n",
    "# │       ├── room_dropout_noise\n",
    "# │       └── ws_dropout_noise\n",
    "# ├── preprocessed\n",
    "# │   ├── graph matching\n",
    "# │   │   ├── equal\n",
    "# │   │   ├── global\n",
    "# │   │   ├── global_local\n",
    "# │   │   └── local\n",
    "# │   └── partial graph matching\n",
    "# │       ├── room_dropout_equal\n",
    "# │       ├── ws_dropout_equal\n",
    "# │       ├── room_dropout_noise\n",
    "# │       └── ws_dropout_noise\n",
    "# └── raw\n",
    "#     ├── graph matching\n",
    "#     │   ├── equal\n",
    "#     │   ├── global\n",
    "#     │   ├── global_local\n",
    "#     │   └── local\n",
    "#     └── partial graph matching\n",
    "#         ├── room_dropout_equal\n",
    "#         ├── ws_dropout_equal\n",
    "#         ├── room_dropout_noise\n",
    "#         └── ws_dropout_noise\n",
    "\n",
    "def create_dir_structure(base_dir=\"GNN\"):\n",
    "    categories = [\n",
    "        \"graph_matching/equal\",\n",
    "        \"graph_matching/global\",\n",
    "        \"graph_matching/global_local\",\n",
    "        \"graph_matching/local\",\n",
    "        \"partial_graph_matching/room_dropout_equal\",\n",
    "        \"partial_graph_matching/ws_dropout_equal\",\n",
    "        \"partial_graph_matching/room_dropout_noise\",\n",
    "        \"partial_graph_matching/ws_dropout_noise\",\n",
    "    ]\n",
    "\n",
    "    levels = [\"models\", \"preprocessed\", \"raw\"]\n",
    "\n",
    "    for level in levels:\n",
    "        for category in categories:\n",
    "            path = os.path.join(base_dir, level, category)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dir_structure(GNN_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Matching dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wL_59NMdsZW"
   },
   "source": [
    "### GM Equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-voxq-c7dsZW"
   },
   "outputs": [],
   "source": [
    "# graph matching-equal path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "original_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C067q1ZRdsZW",
    "outputId": "92173103-00e3-4ee5-f638-3e74bb3e0647"
   },
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(original_graphs)}\")\n",
    "print(original_graphs[0])\n",
    "print(original_graphs[0].nodes(data=True))\n",
    "print(original_graphs[0].edges(data=True))\n",
    "plot_a_graph([original_graphs[0]], path=os.path.join(gm_path, \"equal\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txNnbzQNdsZX"
   },
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC9jsHBVdsZX",
    "outputId": "d756beb8-946c-47e6-c331-3172b5858fad"
   },
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, g1, pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    original_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "\n",
    "# Visualize the two graphs\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDBrICr9dsZY"
   },
   "source": [
    "### GM Local Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr73ZwdNdsZZ"
   },
   "outputs": [],
   "source": [
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5Hv04_zdsZZ",
    "outputId": "ce138f8f-c426-4fb0-94ca-e60d1f91ef06"
   },
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]], path=os.path.join(gm_path, \"local\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxYqO6uWdsZa"
   },
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8cG0CBKdsZa",
    "outputId": "7dbd9168-507f-4684-fa4e-000a1617a6d2"
   },
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"local\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "# Visualize the two graphs\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm2RdRK5dsZa"
   },
   "source": [
    "### GM Global Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvDynD3edsZb"
   },
   "outputs": [],
   "source": [
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWpxuPk0dsZc",
    "outputId": "e7a56a77-c751-40b6-c0b5-767f62f5fdb4"
   },
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]], path=os.path.join(gm_path, \"global\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT7-XrN8dsZc"
   },
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjXVVYCEdsZd",
    "outputId": "fab19d62-fadc-4f66-a3d8-2a6ad37a0743"
   },
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"global\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "# Visualize the two graphs\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ih7g4VddsZd"
   },
   "source": [
    "### GM Global + Local Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzwL4zxTdsZe"
   },
   "outputs": [],
   "source": [
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"global_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTgPcgp_dsZe",
    "outputId": "2e17bd08-bd78-44c8-f8cc-c089ce81db11"
   },
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]], path=os.path.join(gm_path, \"global_local\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKHJVylIdsZf"
   },
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oLVa11idsZf",
    "outputId": "6a9a1c49-3134-47b4-9964-6d3ee43c34e3"
   },
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"global_local\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "# Visualize the two graphs\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Graph Matching dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS dropout equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph matching-equal path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "original_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"equal\")\n",
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"partial_graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"ws_dropout_equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]],path=os.path.join(gm_path, \"ws_dropout_equal\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"ws_dropout_equal\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS dropout noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph matching-equal path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "original_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"equal\")\n",
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"partial_graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"ws_dropout_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[9]],path=os.path.join(gm_path, \"ws_dropout_noise\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"ws_dropout_noise\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "g1_out, g2_perm, gt_perm = train[5]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Room dropout equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph matching-equal path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "original_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"equal\")\n",
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"partial_graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"room_dropout_equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]],path=os.path.join(gm_path, \"room_dropout_equal\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"room_dropout_equal\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Room dropout noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph matching-equal path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"graph_matching\")\n",
    "original_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"equal\")\n",
    "# graph matching path\n",
    "gm_path = os.path.join(GNN_PATH, \"raw\", \"partial_graph_matching\")\n",
    "noise_graphs, _, _ = deserialize_MSD_dataset(data_path=gm_path, original_path=\"room_dropout_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of graphs\n",
    "print(f\"Number of original graphs: {len(noise_graphs)}\")\n",
    "print(noise_graphs[0])\n",
    "print(noise_graphs[0].nodes(data=True))\n",
    "print(noise_graphs[0].edges(data=True))\n",
    "plot_a_graph([noise_graphs[0]],path=os.path.join(gm_path, \"room_dropout_noise\", \"apartment.png\"), viz_rooms=True, viz_ws=True, viz_openings=False, viz_room_connection=True, viz_normals=False, viz_room_normals=True, viz_walls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate G1,G2,GT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_gt_list = []\n",
    "for i, g1 in enumerate(original_graphs):\n",
    "    generate_matching_pair_as_data(g1, noise_graphs[i], pair_gt_list)\n",
    "\n",
    "train, val, test = split_graphs_stratified(pair_gt_list)\n",
    "\n",
    "describe(train, \"TRAIN\")\n",
    "describe(val,   \"VAL\")\n",
    "describe(test,  \"TEST\")\n",
    "\n",
    "# compute mean and std\n",
    "mean, std = compute_mean_std(train)\n",
    "\n",
    "# Normalizzazione dei set\n",
    "train_pairs_norm = normalize_data_pairs(train, mean, std)\n",
    "val_pairs_norm = normalize_data_pairs(val, mean, std)\n",
    "test_pairs_norm = normalize_data_pairs(test, mean, std)\n",
    "\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"room_dropout_noise\")\n",
    "serialize_graph_matching_dataset(\n",
    "    train_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    val_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    test_pairs_norm,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")\n",
    "serialize_graph_matching_dataset(\n",
    "    noise_graphs,\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "g1_out, g2_perm, gt_perm = train[0]\n",
    "\n",
    "print(g1_out)\n",
    "print(\"G1 nodes:\", g1_out.x[0])\n",
    "print(g2_perm)\n",
    "print(\"G2 permuted nodes:\", g2_perm.x[0])\n",
    "print(\"Ground truth permutation:\\n\", gt_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two graphs\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=gt_perm,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(gm_equal_preprocessed_path, \"gt.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_XTk3X9dsZV"
   },
   "source": [
    "# Graph matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4EdN2opdsZf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Equal graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML4uKWG6dsZg",
    "outputId": "fb598dac-68bc-4cf1-be6d-7b2515082bfb"
   },
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', 'graph_matching', 'equal')\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5FE_ULjdsZg",
    "outputId": "1e4727f4-51c8-4c06-dc8a-b19c13b30d45"
   },
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iIhvpIEdsZg"
   },
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8Q-TdAwdsZh",
    "outputId": "34f61cf7-042c-4a78-d225-f8087844d31f"
   },
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2Sinkhorn(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"GM_equal\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV-pTNGNdsZi",
    "outputId": "4c3d20dc-91bf-45b8-9cdd-752dde65a066"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hwnmcy8dsZi",
    "outputId": "e507cdb0-3184-4452-9237-d90c18500ecf"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QHU0aZ-dsZj",
    "outputId": "5d3ffa20-fc79-4279-b57d-942416da5e50"
   },
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[236]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"all\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UF3A67XdsZj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Local noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ1u7aCedsZj",
    "outputId": "a4e95313-546c-4e5b-e313-273a116bdf41"
   },
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"local\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', 'graph_matching', 'local')\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt4rX3pBdsZk",
    "outputId": "4ed3571a-2df9-490f-fd1e-4264e9a62847"
   },
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmRO1JSodsZk"
   },
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9oIucY9dsZl",
    "outputId": "778692cc-114d-4326-e6c0-fbcfd41e611c"
   },
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2Sinkhorn(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"GM_local\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivzH9FuUdsZl",
    "outputId": "45ec61ec-1d89-4e15-985d-275a33c095fe"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w4aanj7dsZm",
    "outputId": "6f119110-4989-4082-cf9e-43b1001e9194"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQuRATYWdsZm",
    "outputId": "eb1a2fbf-9d59-474a-9d00-076837879026"
   },
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[586]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gch5GNBNdsZm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Global noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2zESEQodsZn",
    "outputId": "7d8ff7a6-ebe2-4f22-c09a-2bca30bc764b"
   },
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"global\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', 'graph_matching', 'global')\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEOkiXDKdsZn",
    "outputId": "bf78e9ad-46ac-4dbb-ac11-2a3bbcc46236"
   },
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnstUFO0dsZn"
   },
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNqgzChNdsZo",
    "outputId": "e989ac19-645a-46ed-ca7c-0e6dee09521b"
   },
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2Sinkhorn(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"GM_global\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt9jD976dsZo",
    "outputId": "686e8e71-156d-4a75-e35a-dc24bf5ac427"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzGHyR7XdsZp",
    "outputId": "dd3520c5-1e50-4de8-a812-72a4d1840ce6"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DMHg-WBdsZp",
    "outputId": "bab4b0e7-4744-4b6b-ce21-efd4e5237d0f"
   },
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[39]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivsyBkoTdsZp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Global + Local noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi34oY4ddsZq",
    "outputId": "23516bc1-b8dc-4092-a9e8-ccb86298c6c9"
   },
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"global_local\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', 'graph_matching', 'global_local')\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZoeGiMLdsZq",
    "outputId": "00a9a594-3e47-4543-e087-94d1a8f1c938"
   },
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIlYmrxadsZq"
   },
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fd8vviCMdsZs",
    "outputId": "768f8fe0-de43-4576-f7c1-cb756b7c236c"
   },
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2Sinkhorn(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"GM_global_local\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize initial validation embeddings\n",
    "batch = next(iter(val_loader))\n",
    "h1_val, h2_val = batch[0], batch[1]\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_room.png'), node_type_filter=\"room\")\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_ws.png'), node_type_filter=\"ws\")\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_all.png'), node_type_filter=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001, node_type_filter=\"all\")\n",
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution_room.gif\"), fps=0.001, node_type_filter=\"room\")\n",
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution_ws.gif\"), fps=0.001, node_type_filter=\"ws\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1hwMXy0dsZt",
    "outputId": "bed82726-b546-49a4-a261-32733fefd5c6"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6H7I20sdsZv",
    "outputId": "2b6171bf-09ff-4d59-b77f-6be6bd9f89aa"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ereHBI3dsZv",
    "outputId": "f6cbce29-0a3e-44db-cb3f-3b2ea2d3dce1"
   },
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[25]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=True)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE4_rdTCdsZv"
   },
   "source": [
    "# Partial graph matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9cx0sDmdsZw"
   },
   "source": [
    "## Ws dropout equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4895 pairs from ./GNN/preprocessed/graph_matching/equal/original.pkl\n",
      "Loaded 4895 pairs from ./GNN/preprocessed/partial_graph_matching/ws_dropout_equal/noise.pkl\n",
      "Loaded 3426 pairs from ./GNN/preprocessed/partial_graph_matching/ws_dropout_equal/train_dataset.pkl\n",
      "Loaded 734 pairs from ./GNN/preprocessed/partial_graph_matching/ws_dropout_equal/valid_dataset.pkl\n",
      "Loaded 735 pairs from ./GNN/preprocessed/partial_graph_matching/ws_dropout_equal/test_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"ws_dropout_equal\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', \"partial_graph_matching\", \"ws_dropout_equal\")\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[57, 7], edge_index=[2, 109], name='1575', node_names=[57], permutation=[57])\n",
      "Data(x=[43, 7], edge_index=[2, 71], name='1575', node_names=[43], permutation=[43])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([57, 43])\n"
     ]
    }
   ],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "print(gt.shape)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatchingModel_GATv2SinkhornTopK(\n",
      "  (gnn): ModuleList(\n",
      "    (0): GATv2Conv(7, 64, heads=1)\n",
      "    (1): GATv2Conv(64, 32, heads=1)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (inst_norm): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      ")\n",
      "Number of parameters: 5378\n"
     ]
    }
   ],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2SinkhornTopK(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"PGM_ws_dropout_equal\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize initial validation embeddings\n",
    "batch = next(iter(val_loader))\n",
    "h1_val, h2_val = batch[0], batch[1]\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_room.png'), node_type_filter=\"room\")\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_ws.png'), node_type_filter=\"ws\")\n",
    "visualize_initial_embeddings(h1_val.x, h2_val.x, os.path.join(models_path, 'initial_all.png'), node_type_filter=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[3]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ws dropout noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"ws_dropout_noise\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', \"partial_graph_matching\", \"ws_dropout_noise\")\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "print(gt.shape)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2SinkhornTopK(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"PGM_ws_dropout_noise\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[3]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Room dropout equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"room_dropout_equal\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', \"partial_graph_matching\", \"room_dropout_equal\")\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "print(gt.shape)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2SinkhornTopK(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"PGM_room_dropout_equal\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[3]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Room dropout noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessed dataset\n",
    "gm_equal_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"graph_matching\", \"equal\")\n",
    "gm_local_preprocessed_path = os.path.join(GNN_PATH, \"preprocessed\", \"partial_graph_matching\", \"room_dropout_noise\")\n",
    "models_path = os.path.join(GNN_PATH, 'models', \"partial_graph_matching\", \"room_dropout_noise\")\n",
    "\n",
    "original_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_equal_preprocessed_path,\n",
    "    \"original.pkl\"\n",
    ")\n",
    "noise_graphs = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"noise.pkl\"\n",
    ")\n",
    "\n",
    "train_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"train_dataset.pkl\"\n",
    ")\n",
    "val_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"valid_dataset.pkl\"\n",
    ")\n",
    "test_list = deserialize_graph_matching_dataset(\n",
    "    gm_local_preprocessed_path,\n",
    "    \"test_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1,d2,gt = train_list[0]\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(gt)\n",
    "print(gt.shape)\n",
    "plot_two_graphs_with_matching([d1,d2],gt_perm=gt,original_graphs=original_graphs,noise_graphs=noise_graphs,path=os.path.join(models_path, \"train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphMatchingDataset(train_list)\n",
    "val_dataset = GraphMatchingDataset(val_list)\n",
    "test_dataset = GraphMatchingDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per salvare i modelli\n",
    "best_val_model_path = os.path.join(models_path, 'best_val_model.pt')\n",
    "final_model_path = os.path.join(models_path, 'final_model.pt')\n",
    "\n",
    "# Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_pyg_matching, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pyg_matching)\n",
    "\n",
    "# Modello e ottimizzatore\n",
    "model = MatchingModel_GATv2SinkhornTopK(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=out_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Logger TensorBoard\n",
    "writer = setup_tb_logger(\n",
    "    base_dir=\"tb_logs\",\n",
    "    model_name=model._get_name(),\n",
    "    dataset_name=\"PGM_room_dropout_noise\",\n",
    "    experiment_name=\"exp1\"\n",
    ")\n",
    "\n",
    "#model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_embeddings_history = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    writer=writer,\n",
    "    best_model_path=best_val_model_path,\n",
    "    final_model_path=final_model_path,\n",
    "    patience=patience,\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_gif_stride(val_embeddings_history, os.path.join(models_path, \"embeddings_evolution.gif\"), fps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, os.path.join(models_path, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_val_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, test_loss, test_embeddings = evaluate_sinkhorn(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "inference_times = []\n",
    "# use the model to predict the matching on a test graph\n",
    "correct = 0\n",
    "total_cols = 0\n",
    "\n",
    "for i, (g1_out, g2_perm, gt_perm) in enumerate(test_list):\n",
    "    start_time = time.time()\n",
    "    result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "    errors = (result != gt_perm.to(result.device)).sum().item()\n",
    "    if errors > 0:\n",
    "        print(f\"Graph {i}: Errors found: {errors}\")\n",
    "\n",
    "    # Accuracy calculation after hungarian\n",
    "    pred_idx = result.argmax(dim=0)\n",
    "    target_idx = gt_perm.argmax(dim=0)\n",
    "    correct += (pred_idx == target_idx).sum().item()\n",
    "    total_cols += result.shape[1]\n",
    "\n",
    "accuracy = correct / total_cols if total_cols > 0 else 0.0\n",
    "print(f\"Test Accuracy (after Hungarian): {accuracy:.4f}\")\n",
    "\n",
    "mean_inference_time = np.mean(inference_times)\n",
    "std_inference_time = np.std(inference_times)\n",
    "print(f\"Inference time: {mean_inference_time:.6f} seconds (mean) ± {std_inference_time:.6f} seconds (std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_out, g2_perm, gt_perm = test_list[3]\n",
    "result = predict_matching_matrix(model, g1_out, g2_perm, use_hungarian=False)\n",
    "\n",
    "plot_two_graphs_with_matching(\n",
    "    [g1_out, g2_perm],\n",
    "    gt_perm=gt_perm,\n",
    "    pred_perm=result,\n",
    "    original_graphs=original_graphs,\n",
    "    noise_graphs=noise_graphs,\n",
    "    viz_rooms=True,\n",
    "    viz_ws=True,\n",
    "    match_display=\"wrong\",\n",
    "    path=os.path.join(models_path, \"test.png\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
